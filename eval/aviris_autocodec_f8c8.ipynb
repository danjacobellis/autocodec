{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b4dc22-bdb4-4327-8964-869e0631a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2 import PILToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7672d-c57e-42b5-9c96-fff6fa841f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "aviris = datasets.load_dataset(\"danjacobellis/aviris_1k_val\", split=\"validation\")\n",
    "\n",
    "checkpoint = torch.load('../../hf/autocodec/hyper_f8c8.pth', map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J=int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth=config.encoder_depth,\n",
    "    encoder_kernel_size=config.encoder_kernel_size,\n",
    "    decoder_depth=config.decoder_depth,\n",
    "    lightweight_encode=config.lightweight_encode,\n",
    "    lightweight_decode=config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677d531-b5cc-409d-ab08-498aa0fc354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiff_to_tensor(img: PIL.Image.Image) -> torch.Tensor:\n",
    "    \"\"\"Convert multi‑frame TIFF to (C,H,W) float tensor in [-1,1].\"\"\"\n",
    "    bands = []\n",
    "    for i_band in range(img.n_frames):\n",
    "        img.seek(i_band)\n",
    "        bands.append(np.array(img, dtype='int16'))\n",
    "    return torch.tensor(np.stack(bands), dtype=torch.float32) / 32768.0\n",
    "\n",
    "\n",
    "def evaluate_quality(sample):\n",
    "    img = sample['image']\n",
    "    x_orig = tiff_to_tensor(img).unsqueeze(0).to(device).to(torch.bfloat16).clamp(-1, 1)\n",
    "    voxels = x_orig.numel()\n",
    "\n",
    "    # --- Encode ---\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x_orig)\n",
    "        latent = model.quantize.compand(z).round()\n",
    "    latent_imgs = latent_to_pil(latent.cpu(), n_bits=8, C=config.latent_dim)\n",
    "    size_bytes = 0\n",
    "    for im in latent_imgs:\n",
    "        buff = io.BytesIO()\n",
    "        im.save(buff, format='PNG')  # lossless\n",
    "        size_bytes += len(buff.getbuffer())\n",
    "    encode_time = time.time() - t0\n",
    "\n",
    "    # --- Decode ---\n",
    "    t0 = time.time()\n",
    "    latent_decoded = pil_to_latent(latent_imgs, N=config.latent_dim, n_bits=8, C=config.latent_dim).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = model.decode(latent_decoded).clamp(-1, 1)\n",
    "    decode_time = time.time() - t0\n",
    "\n",
    "    # --- Metrics ---\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = (-10 * mse.log10()).item()\n",
    "\n",
    "    bpv = 8 * size_bytes / voxels  # bits per voxel\n",
    "\n",
    "    return {\n",
    "        'voxels': voxels,\n",
    "        'encode_time': encode_time,\n",
    "        'decode_time': decode_time,\n",
    "        'bpv': bpv,\n",
    "        'PSNR': PSNR,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c4f55-0ba5-411a-add1-eeb92a7fb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'voxels',\n",
    "    'encode_time',\n",
    "    'decode_time',\n",
    "    'bpv',\n",
    "    'PSNR',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4cfb7-ace4-447a-87f7-f18b302e77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset = aviris.map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a27b6-60a1-4b9d-bfd7-cee0e2573495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in metrics:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")\n",
    "print(f\"{np.mean(np.array(results_dataset['voxels'])/1e6/np.array(results_dataset['encode_time']))} MVox/sec\")\n",
    "print(f\"{np.mean(np.array(results_dataset['voxels'])/1e6/np.array(results_dataset['decode_time']))} MVox/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
