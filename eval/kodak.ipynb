{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb7d1ed-cfe7-4a60-a5ef-4e7083205b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cfb386c-17b8-4006-950e-b976c7341027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c48_ft.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06d2d79-fd36-419d-bfc8-96579af4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64a05a4-fbef-48b7-956b-bd96baec68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d30e515-2200-47cd-966c-f246ff51679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.7802615695529515\n",
      "PSNR: 31.03515625\n",
      "LPIPS_dB: 7.04490827424515\n",
      "DISTS_dB: 13.877594294056875\n",
      "SSIM: 0.8351236979166666\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42778a84-d8ea-44ff-baf7-11a75a462378",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ddbd71-9763-42cf-8f2c-84fbcdb19d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd3d5ab-6cd8-41a7-870b-5d1129abc942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db36d61adc1451b873ef5214a476d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rgb_f16c48.pth:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c48.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90cb27e-7bd1-491e-870d-7a4573abb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a3e16f-fe4e-49d9-a945-04d38d701367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3313d7389469491d94628424cc015cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bb3030-5003-4c5f-99c7-7e5b61f9dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.6455654568142362\n",
      "PSNR: 30.846354166666668\n",
      "LPIPS_dB: 6.750272797771676\n",
      "DISTS_dB: 13.422796385256314\n",
      "SSIM: 0.82958984375\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65a105-5f42-4227-9708-110c7cf2ab5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a6f2dd-d1dd-4531-91fc-d1597d83b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77768893-12a8-4a4c-82c7-5a91f530ca67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf57380c02f4dc299d8c829868f284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rgb_f16c12_ft.pth:  75%|#######5  | 231M/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c12_ft.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb3fcfc-2cc6-46e4-b9a7-b7a86593648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9bc36bd-4073-4298-9f12-3eb0851a8661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b628d1813804853b1ef8d5f1bf33b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cef8466-2403-4d89-8ace-36abb3296e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.18493143717447916\n",
      "PSNR: 26.97265625\n",
      "LPIPS_dB: 4.7562505625050795\n",
      "DISTS_dB: 9.379736408196386\n",
      "SSIM: 0.7394205729166666\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15469f2-0297-4b07-8194-f56d6e6ca646",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5d5dfd-ac18-4d59-a2e2-1269d14057e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae270b4-b088-453c-ae22-ca5fbd8d2a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1699aee1b66e4f77af152a17defb91ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rgb_f16c12.pth:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c12.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46408e7e-b449-43fe-8d7e-720b648e7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97048e8-0a8e-40a4-ab08-41ea9a99856b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3434fe425b7f4af192956ff7b0256f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a272eed-c37f-4e47-8a4b-01a42d0bc7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.15073140462239584\n",
      "PSNR: 26.770833333333332\n",
      "LPIPS_dB: 4.522872237559315\n",
      "DISTS_dB: 8.996013821472557\n",
      "SSIM: 0.7294921875\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
