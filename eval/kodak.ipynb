{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ea0c11-9e4f-4ad2-8abd-82a3a685f4f2",
   "metadata": {},
   "source": [
    "| Model (training dataset)             |   bpp   |  PSNR  | LPIPS (dB) | DISTS (dB) |  SSIM  |\n",
    "|--------------------------------------|--------:|-------:|-----------:|-----------:|-------:|\n",
    "| Cosmos di16×16 (Proprietary)         | 0.0625  | 21.7743 | 5.3784     | 10.6189    | 0.6449 |\n",
    "| LiVeAction f16c12 (LSDIR)            | 0.1507  | 26.7708 | 4.5229     |  8.9960    | 0.7295 |\n",
    "| LiVeAction f16c12 (ImageNet)         | 0.1849  | 26.9727 | 4.7563     |  9.3797    | 0.7394 |\n",
    "| Cosmos di8×8 (Proprietary)           | 0.2500  | 25.9193 | 7.7112     | 13.2647    | 0.8558 |\n",
    "| WaLLoC f8c12 (LSDIR)                 | 0.6171  | 30.5576 | 6.5138     | 13.2437    | 0.9501 |\n",
    "| LiVeAction f16c48 (LSDIR)            | 0.6456  | 30.8464 | 6.7503     | 13.4228    | 0.8296 |\n",
    "| LiVeAction f16c48 (ImageNet)         | 0.7803  | 31.0352 | 7.0449     | 13.8776    | 0.8351 |\n",
    "| WaLLoC f8c48 (LSDIR)                 | 2.5436  | 37.3370 | 11.6739    | 18.2942    | 0.9873 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb7d1ed-cfe7-4a60-a5ef-4e7083205b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cfb386c-17b8-4006-950e-b976c7341027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c48_ft.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06d2d79-fd36-419d-bfc8-96579af4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64a05a4-fbef-48b7-956b-bd96baec68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d30e515-2200-47cd-966c-f246ff51679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.7802615695529515\n",
      "PSNR: 31.03515625\n",
      "LPIPS_dB: 7.04490827424515\n",
      "DISTS_dB: 13.877594294056875\n",
      "SSIM: 0.8351236979166666\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42778a84-d8ea-44ff-baf7-11a75a462378",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ddbd71-9763-42cf-8f2c-84fbcdb19d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd3d5ab-6cd8-41a7-870b-5d1129abc942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db36d61adc1451b873ef5214a476d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rgb_f16c48.pth:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c48.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90cb27e-7bd1-491e-870d-7a4573abb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a3e16f-fe4e-49d9-a945-04d38d701367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3313d7389469491d94628424cc015cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bb3030-5003-4c5f-99c7-7e5b61f9dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.6455654568142362\n",
      "PSNR: 30.846354166666668\n",
      "LPIPS_dB: 6.750272797771676\n",
      "DISTS_dB: 13.422796385256314\n",
      "SSIM: 0.82958984375\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65a105-5f42-4227-9708-110c7cf2ab5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a6f2dd-d1dd-4531-91fc-d1597d83b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77768893-12a8-4a4c-82c7-5a91f530ca67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf57380c02f4dc299d8c829868f284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rgb_f16c12_ft.pth:  75%|#######5  | 231M/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c12_ft.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb3fcfc-2cc6-46e4-b9a7-b7a86593648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9bc36bd-4073-4298-9f12-3eb0851a8661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b628d1813804853b1ef8d5f1bf33b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cef8466-2403-4d89-8ace-36abb3296e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.18493143717447916\n",
      "PSNR: 26.97265625\n",
      "LPIPS_dB: 4.7562505625050795\n",
      "DISTS_dB: 9.379736408196386\n",
      "SSIM: 0.7394205729166666\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15469f2-0297-4b07-8194-f56d6e6ca646",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5d5dfd-ac18-4d59-a2e2-1269d14057e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae270b4-b088-453c-ae22-ca5fbd8d2a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1699aee1b66e4f77af152a17defb91ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rgb_f16c12.pth:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c12.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46408e7e-b449-43fe-8d7e-720b648e7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.bfloat16) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97048e8-0a8e-40a4-ab08-41ea9a99856b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3434fe425b7f4af192956ff7b0256f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a272eed-c37f-4e47-8a4b-01a42d0bc7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.15073140462239584\n",
      "PSNR: 26.770833333333332\n",
      "LPIPS_dB: 4.522872237559315\n",
      "DISTS_dB: 8.996013821472557\n",
      "SSIM: 0.7294921875\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7253f3-634a-460a-b2f6-60aaa89fd499",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7df01c5-9f20-48fc-b69f-ff7451bdcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np, json\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from walloc import walloc\n",
    "from walloc.walloc import latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364b9757-182b-4207-b8d6-743f9c1be482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "config_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_16x.json\"\n",
    ")\n",
    "codec_config = SimpleNamespace(**json.load(open(config_file)))\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_16x.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "codec = walloc.Codec2D(\n",
    "    channels = codec_config.channels,\n",
    "    J = codec_config.J,\n",
    "    Ne = codec_config.Ne,\n",
    "    Nd = codec_config.Nd,\n",
    "    latent_dim = codec_config.latent_dim,\n",
    "    latent_bits = codec_config.latent_bits,\n",
    "    lightweight_encode = codec_config.lightweight_encode\n",
    ")\n",
    "codec.load_state_dict(checkpoint['model_state_dict'])\n",
    "codec = codec.to(device)\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a099a85-6f51-4ea0-b195-4625dfcf1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.float) / 255 - 0.5\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encoder(codec.wavelet_analysis(x_orig,J=codec.J))\n",
    "    webp = latent_to_pil(z.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=codec_config.latent_dim, n_bits=8, C=3).to(device).to(torch.float16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.wavelet_synthesis(codec.decoder(z),J=codec.J).clamp(-0.5,0.5)\n",
    "    x_orig_01 = x_orig + 0.5\n",
    "    x_hat_01 = x_hat + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692f32be-2b45-4c9d-9c69-117665fd8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2b985cbf9415b912b0be7df3cf617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79bcb15-704e-4e50-b9bb-b152675e2c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.6171129014756944\n",
      "PSNR: 30.557596782843273\n",
      "LPIPS_dB: 6.513779126102302\n",
      "DISTS_dB: 13.243720676934606\n",
      "SSIM: 0.950053483247757\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53882a-7bdb-4834-b7b1-422ecca91ecd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cd9e47-3171-43f3-a4a3-e6998df139fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np, json\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from walloc import walloc\n",
    "from walloc.walloc import latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddb5ae3-a62e-4361-a029-e1518d98ce2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472aad8c073e4b04aeff20d0c98e6063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RGB_4x.json:   0%|          | 0.00/388 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a832d2f3294369b77cbf1291c13068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RGB_4x.pth:   0%|          | 0.00/229M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "config_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_4x.json\"\n",
    ")\n",
    "codec_config = SimpleNamespace(**json.load(open(config_file)))\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_4x.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "codec = walloc.Codec2D(\n",
    "    channels = codec_config.channels,\n",
    "    J = codec_config.J,\n",
    "    Ne = codec_config.Ne,\n",
    "    Nd = codec_config.Nd,\n",
    "    latent_dim = codec_config.latent_dim,\n",
    "    latent_bits = codec_config.latent_bits,\n",
    "    lightweight_encode = codec_config.lightweight_encode\n",
    ")\n",
    "codec.load_state_dict(checkpoint['model_state_dict'])\n",
    "codec = codec.to(device)\n",
    "codec.eval();\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6657a663-9d3f-4dfa-8a64-2e766453e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.float) / 255 - 0.5\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = codec.encoder(codec.wavelet_analysis(x_orig,J=codec.J))\n",
    "    webp = latent_to_pil(z.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    latent_decoded = pil_to_latent(webp, N=codec_config.latent_dim, n_bits=8, C=3).to(device).to(torch.float16)\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.wavelet_synthesis(codec.decoder(z),J=codec.J).clamp(-0.5,0.5)\n",
    "    x_orig_01 = x_orig + 0.5\n",
    "    x_hat_01 = x_hat + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4d18bf-982a-49b1-8c87-376a2e675516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ce806461c4410c8e9c0b88bda9035b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bca37c8-d41f-416a-aba1-c5e2c94760d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 2.5435909695095487\n",
      "PSNR: 37.337041000525154\n",
      "LPIPS_dB: 11.673939380783379\n",
      "DISTS_dB: 18.29417419215874\n",
      "SSIM: 0.9873491401473681\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a66791-0854-47de-918e-0e25106e4076",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b7ea2e-cd76-47b4-9e1a-374dc68a0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import snapshot_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c374cd5a-1e5d-4197-a2b3-f031e3c5d98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b8f0fee76246718c72b5413fbfac34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6bc0a94afa42dab91e792c16c772ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "autoencoder.jit:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62edb47d2e8487a9035eb0a1a3bb2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder.jit:   0%|          | 0.00/94.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c18195dfdb84a45a4d23e8b417f1e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0e6989eeff46f196d4449c7ab8bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec8cf0cf7b64fde87522a7ce1a1e09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoder.jit:   0%|          | 0.00/65.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5c0bb7ed594322a55c232994b6edbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe408fbafbb940e6bdff375bbf88aa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_config.yaml:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "model_path = snapshot_download(repo_id='nvidia/Cosmos-Tokenizer-DI8x8')\n",
    "encoder = ImageTokenizer(checkpoint_enc=f'{model_path}/encoder.jit').to(device)\n",
    "decoder = ImageTokenizer(checkpoint_dec=f'{model_path}/decoder.jit').to(device)\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92dd97ba-6973-4591-94b8-ed7c96eb4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.float) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = encoder.encode(x_orig)[0]\n",
    "    size_bytes = 2*z.numel()\n",
    "    with torch.no_grad():\n",
    "        x_hat = decoder.decode(z).to(torch.float).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab649598-9140-4acb-83fc-37da832d5a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function evaluate_quality at 0x748ae421e830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d495e34283134ac69f9bc2c783ceb73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c966de66-7e9f-4cf1-8f83-6105327b7bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.25\n",
      "PSNR: 25.919317603111267\n",
      "LPIPS_dB: 7.711161962844201\n",
      "DISTS_dB: 13.264738029047363\n",
      "SSIM: 0.8557981674869856\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c6339e-ad0a-4c18-88fb-bb3b85f0fb9b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eda25f3-a1d1-49e3-9073-f905b1a17bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np\n",
    "from huggingface_hub import snapshot_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d4ade2-8d2f-48fa-86a3-2f9e28c66e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80185224e9214f7d8567a7719d7328fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "model_path = snapshot_download(repo_id='nvidia/Cosmos-Tokenizer-DI16x16')\n",
    "encoder = ImageTokenizer(checkpoint_enc=f'{model_path}/encoder.jit').to(device)\n",
    "decoder = ImageTokenizer(checkpoint_dec=f'{model_path}/decoder.jit').to(device)\n",
    "\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939473de-df1e-4756-8ac7-b256a485e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):\n",
    "    img = sample['image'].convert(\"RGB\")\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.float) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "    with torch.no_grad():\n",
    "        z = encoder.encode(x_orig)[0]\n",
    "    size_bytes = 2*z.numel()\n",
    "    with torch.no_grad():\n",
    "        x_hat = decoder.decode(z).to(torch.float).clamp(-1,1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'pixels': pixels,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d06c38d-ac9e-48a4-b3dc-465dffafd3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function evaluate_quality at 0x7a8987f5c160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7040b39f814d4d4c907c8ad8d90e9765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = dataset['validation'].map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae96a9a-335e-4e9e-b185-b30221c2a3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "pixels: 393216.0\n",
      "bpp: 0.0625\n",
      "PSNR: 21.77425762017568\n",
      "LPIPS_dB: 5.378436234862551\n",
      "DISTS_dB: 10.618903597362483\n",
      "SSIM: 0.6448745379845301\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'pixels',\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
