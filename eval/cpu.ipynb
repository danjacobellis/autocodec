{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e441ad02-4c37-4207-8d36-c9e9e2cd4b8f",
   "metadata": {},
   "source": [
    "Music (stereo) – Stable Audio\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1f5a65-4d9d-4c41-b7ad-80c194133af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/home/dgj335/.local/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 4096; 88.73395201718911\n",
      "L: 65536; 229.36807058662532\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers.models.autoencoders import AutoencoderOobleck\n",
    "codec = AutoencoderOobleck.from_pretrained(\n",
    "    \"stabilityai/stable-audio-open-1.0\",\n",
    "    subfolder='vae',\n",
    "    torch_dtype=torch.float\n",
    ")\n",
    "codec.eval();\n",
    "\n",
    "for L in [2**12, 2**16]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn((1,2,L)).clamp(-1,1).to(torch.float)\n",
    "        t0 = time.time()\n",
    "        z = codec.encode(x).latent_dist.mode().to(torch.float16).to(\"cpu\")\n",
    "        torch.save(z,'temp.pth')\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'L: {L}; {L/np.median(encode_time)/1e3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a770b23-a736-4c88-9ad4-9934f5a14d61",
   "metadata": {},
   "source": [
    "---\n",
    "Music (stereo) – LiveAction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af2f45a-abeb-4faf-af81-2a95682cbdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 4096; 323.75752268958234\n",
      "L: 65536; 5012.178748842129\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import einops\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset, Image\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = 'cpu'\n",
    "checkpoint = torch.load('../../hf/autocodec/musdb_stereo_f512c16.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=1,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for L in [2**12, 2**16]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn((1,2,L)).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "        z = model.quantize.compand(model.encode(x)).round().cpu()\n",
    "        latent_img = latent_to_pil(z.unsqueeze(0), n_bits=8, C=1)\n",
    "        buff = io.BytesIO()\n",
    "        latent_img[0].save(buff, format='TIFF', compression='tiff_adobe_deflate')\n",
    "        tiff_bytes = buff.getbuffer()\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'L: {L}; {L/np.median(encode_time)/1e3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bbf90-10d3-4502-887e-de709ea0b1b3",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image – LiveAction F16C48\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ceb80d-0b97-4707-a253-38366fec9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 320, 420); 7.817857842620586\n",
      "Size: (1, 3, 1080, 1920); 13.08464136146449\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/rgb_f16c48_ft.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,3,320,420), (1,3,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(x)\n",
    "            latent = model.quantize.compand(z).round()\n",
    "        webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "        buff = io.BytesIO()\n",
    "        webp[0].save(buff, format='WEBP', lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b4965-b129-481c-8a3b-d4feefd0bd01",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image – LiveAction F16C12\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ec7828-1650-4742-a017-768f1e171847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 320, 420); 7.559027255782769\n",
      "Size: (1, 3, 1080, 1920); 11.896244649342016\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/rgb_f16c12_ft.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,3,320,420), (1,3,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(x)\n",
    "            latent = model.quantize.compand(z).round()\n",
    "        webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "        buff = io.BytesIO()\n",
    "        webp[0].save(buff, format='WEBP', lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26c977-6b12-46e9-b953-8ca7494aea93",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image – JPEG 2000\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96368950-62d5-469c-8520-d718d6a57a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (3, 320, 420); 6.021282316875608\n",
      "Size: (3, 1080, 1920); 6.012107274250979\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "\n",
    "for S in [(3,320,420), (3,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "        buff = io.BytesIO()\n",
    "        img = to_pil_image(x)\n",
    "        img.save(buff, format= \"JPEG2000\", quality_layers=[120])\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0780f0d1-199b-417f-b286-d31029a83760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 1, 224, 224, 224); 6.297860927809051\n",
      "Size: (1, 1, 224, 1024, 1024); 6.4482910390966905\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "\n",
    "for S in [(1,1,224,224,224), (1,1,224,1024,1024)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        img_list = [to_pil_image(x[0,:,i]) for i in range(224)]\n",
    "        buff = []\n",
    "        for img in img_list:\n",
    "            buff.append(io.BytesIO())\n",
    "            img.save(buff[-1], format= \"JPEG2000\", quality_layers=[284])\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153d531-e76b-45b1-bc89-1ae03f4f77fd",
   "metadata": {},
   "source": [
    "---\n",
    "Hyperspectral – LiveAction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e0800b-dbc7-4249-a036-0f79bb19b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 1, 224, 224, 224); 13.758094663558309\n",
      "Size: (1, 1, 224, 1024, 1024); 14.93040220445062\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/hyper_f8c8.pth', map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J=int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth=config.encoder_depth,\n",
    "    encoder_kernel_size=config.encoder_kernel_size,\n",
    "    decoder_depth=config.decoder_depth,\n",
    "    lightweight_encode=config.lightweight_encode,\n",
    "    lightweight_decode=config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,1,224,224,224), (1,1,224,1024,1024)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        z = einops.rearrange(z, 'b c f h w -> (c f) b h w')\n",
    "        img_list = latent_to_pil(z.cpu(), n_bits=8, C=1)\n",
    "        \n",
    "        buff = []\n",
    "        for img in img_list:\n",
    "            buff.append(io.BytesIO())\n",
    "            img.save(buff[-1], format= \"TIFF\", compression='tiff_adobe_deflate')\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129ec9cc-be26-4116-8270-8680bfc2dc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50176"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224*224"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
