{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e441ad02-4c37-4207-8d36-c9e9e2cd4b8f",
   "metadata": {},
   "source": [
    "Music (stereo) - Stable Audio\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1f5a65-4d9d-4c41-b7ad-80c194133af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/home/dgj335/.local/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 4096; 88.73395201718911\n",
      "L: 65536; 229.36807058662532\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers.models.autoencoders import AutoencoderOobleck\n",
    "codec = AutoencoderOobleck.from_pretrained(\n",
    "    \"stabilityai/stable-audio-open-1.0\",\n",
    "    subfolder='vae',\n",
    "    torch_dtype=torch.float\n",
    ")\n",
    "codec.eval();\n",
    "\n",
    "for L in [2**12, 2**16]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn((1,2,L)).clamp(-1,1).to(torch.float)\n",
    "        t0 = time.time()\n",
    "        z = codec.encode(x).latent_dist.mode().to(torch.float16).to(\"cpu\")\n",
    "        torch.save(z,'temp.pth')\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'L: {L}; {L/np.median(encode_time)/1e3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a770b23-a736-4c88-9ad4-9934f5a14d61",
   "metadata": {},
   "source": [
    "---\n",
    "Music (stereo) - LiveAction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af2f45a-abeb-4faf-af81-2a95682cbdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 4096; 323.75752268958234\n",
      "L: 65536; 5012.178748842129\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import einops\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset, Image\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = 'cpu'\n",
    "checkpoint = torch.load('../../hf/autocodec/musdb_stereo_f512c16.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=1,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for L in [2**12, 2**16]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn((1,2,L)).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "        z = model.quantize.compand(model.encode(x)).round().cpu()\n",
    "        latent_img = latent_to_pil(z.unsqueeze(0), n_bits=8, C=1)\n",
    "        buff = io.BytesIO()\n",
    "        latent_img[0].save(buff, format='TIFF', compression='tiff_adobe_deflate')\n",
    "        tiff_bytes = buff.getbuffer()\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'L: {L}; {L/np.median(encode_time)/1e3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bbf90-10d3-4502-887e-de709ea0b1b3",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image - LiveAction F16C48\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ceb80d-0b97-4707-a253-38366fec9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 240, 320); 4.393558754449854\n",
      "Size: (1, 3, 1080, 1920); 10.840091302310277\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/rgb_f16c48_ft.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,3,240,320), (1,3,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(x)\n",
    "            latent = model.quantize.compand(z).round()\n",
    "        webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "        buff = io.BytesIO()\n",
    "        webp[0].save(buff, format='WEBP', lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b4965-b129-481c-8a3b-d4feefd0bd01",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image - LiveAction F16C12\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ec7828-1650-4742-a017-768f1e171847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 240, 320); 5.25159847402915\n",
      "Size: (1, 3, 1080, 1920); 12.283675133820502\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/rgb_f16c12_ft.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,3,240,320), (1,3,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(x)\n",
    "            latent = model.quantize.compand(z).round()\n",
    "        webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "        buff = io.BytesIO()\n",
    "        webp[0].save(buff, format='WEBP', lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478b6a8-af02-4525-b2d4-643961dcd275",
   "metadata": {},
   "source": [
    "---\n",
    "Hyperspectral - JPEG 2000\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0780f0d1-199b-417f-b286-d31029a83760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 1, 224, 224, 224); 6.297860927809051\n",
      "Size: (1, 1, 224, 1024, 1024); 6.4482910390966905\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "\n",
    "for S in [(1,1,224,224,224), (1,1,224,1024,1024)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        img_list = [to_pil_image(x[0,:,i]) for i in range(224)]\n",
    "        buff = []\n",
    "        for img in img_list:\n",
    "            buff.append(io.BytesIO())\n",
    "            img.save(buff[-1], format= \"JPEG2000\", quality_layers=[284])\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153d531-e76b-45b1-bc89-1ae03f4f77fd",
   "metadata": {},
   "source": [
    "---\n",
    "Hyperspectral - LiveAction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e0800b-dbc7-4249-a036-0f79bb19b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 1, 224, 224, 224); 13.758094663558309\n",
      "Size: (1, 1, 224, 1024, 1024); 14.93040220445062\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/hyper_f8c8.pth', map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J=int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth=config.encoder_depth,\n",
    "    encoder_kernel_size=config.encoder_kernel_size,\n",
    "    decoder_depth=config.decoder_depth,\n",
    "    lightweight_encode=config.lightweight_encode,\n",
    "    lightweight_decode=config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,1,224,224,224), (1,1,224,1024,1024)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        z = einops.rearrange(z, 'b c f h w -> (c f) b h w')\n",
    "        img_list = latent_to_pil(z.cpu(), n_bits=8, C=1)\n",
    "        \n",
    "        buff = []\n",
    "        for img in img_list:\n",
    "            buff.append(io.BytesIO())\n",
    "            img.save(buff[-1], format= \"TIFF\", compression='tiff_adobe_deflate')\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f553b-ea18-483e-9290-7de2ebdbd0a0",
   "metadata": {},
   "source": [
    "---\n",
    "Video - LiveAction F8C48\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210e4093-b478-4302-92f2-2a23686c17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 24, 240, 320); 107.623524581751\n",
      "Size: (1, 3, 24, 1080, 1920); 2.386498597486046\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/video_f8c48.pth', map_location=\"cpu\",weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train();\n",
    "\n",
    "for S in [(1,3,24,240,320), (1,3,24,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        for chunk in latent_to_pil(einops.rearrange(z[0], 'c f h w -> f c h w').cpu(),n_bits=8,C=3):\n",
    "            buff = io.BytesIO()\n",
    "            chunk.save(buff,format='webp',lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {24/np.median(encode_time)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b380799-04ea-42c6-a2d1-5e930c4d2491",
   "metadata": {},
   "source": [
    "---\n",
    "Video - LiveAction F8C12\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0dd81d-752a-4d59-9bdf-491518b4e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 24, 240, 320); 108.71682063931951\n",
      "Size: (1, 3, 24, 1080, 1920); 2.399737787287733\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/video_f8c12.pth', map_location=\"cpu\",weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train();\n",
    "\n",
    "for S in [(1,3,24,240,320), (1,3,24,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        for chunk in latent_to_pil(einops.rearrange(z[0], 'c f h w -> f c h w').cpu(),n_bits=8,C=3):\n",
    "            buff = io.BytesIO()\n",
    "            chunk.save(buff,format='webp',lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {24/np.median(encode_time)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
