{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e441ad02-4c37-4207-8d36-c9e9e2cd4b8f",
   "metadata": {},
   "source": [
    "Music (stereo) - Stable Audio\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1f5a65-4d9d-4c41-b7ad-80c194133af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/home/dgj335/.local/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 4096; 88.73395201718911\n",
      "L: 65536; 229.36807058662532\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers.models.autoencoders import AutoencoderOobleck\n",
    "codec = AutoencoderOobleck.from_pretrained(\n",
    "    \"stabilityai/stable-audio-open-1.0\",\n",
    "    subfolder='vae',\n",
    "    torch_dtype=torch.float\n",
    ")\n",
    "codec.eval();\n",
    "\n",
    "for L in [2**12, 2**16]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn((1,2,L)).clamp(-1,1).to(torch.float)\n",
    "        t0 = time.time()\n",
    "        z = codec.encode(x).latent_dist.mode().to(torch.float16).to(\"cpu\")\n",
    "        torch.save(z,'temp.pth')\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'L: {L}; {L/np.median(encode_time)/1e3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a770b23-a736-4c88-9ad4-9934f5a14d61",
   "metadata": {},
   "source": [
    "---\n",
    "Music (stereo) - LiveAction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af2f45a-abeb-4faf-af81-2a95682cbdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 4096; 323.75752268958234\n",
      "L: 65536; 5012.178748842129\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import einops\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset, Image\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = 'cpu'\n",
    "checkpoint = torch.load('../../hf/autocodec/musdb_stereo_f512c16.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=1,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for L in [2**12, 2**16]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn((1,2,L)).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "        z = model.quantize.compand(model.encode(x)).round().cpu()\n",
    "        latent_img = latent_to_pil(z.unsqueeze(0), n_bits=8, C=1)\n",
    "        buff = io.BytesIO()\n",
    "        latent_img[0].save(buff, format='TIFF', compression='tiff_adobe_deflate')\n",
    "        tiff_bytes = buff.getbuffer()\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'L: {L}; {L/np.median(encode_time)/1e3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bbf90-10d3-4502-887e-de709ea0b1b3",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image - LiveAction F16C48\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ceb80d-0b97-4707-a253-38366fec9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 240, 320); 9.699856882170497\n",
      "Size: (1, 3, 1080, 1920); 10.532068901528591\n",
      "Size: (1, 3, 240, 320); 18.55651519096722\n",
      "Size: (1, 3, 1080, 1920); 94.69963060506744\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/autocodec\",\n",
    "    filename=\"rgb_f16c48.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(torch.bfloat16)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval();\n",
    "for (device, dtype)  in [(\"cpu\", torch.float32), (\"cuda\", torch.bfloat16)]:\n",
    "    model.to(device).to(dtype)\n",
    "    for S in [(1,3,240,320), (1,3,1080,1920)]:\n",
    "        encode_time = []\n",
    "        for i_trial in range(101):\n",
    "            x = torch.randn(S,dtype=dtype, device=device).clamp(-1,1)\n",
    "            t0 = time.time()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                z = model.encode(x)\n",
    "                latent = model.quantize.compand(z).round()\n",
    "            webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "            buff = io.BytesIO()\n",
    "            webp[0].save(buff, format='WEBP', lossless=True)\n",
    "            \n",
    "            encode_time.append(time.time() - t0)\n",
    "        print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b4965-b129-481c-8a3b-d4feefd0bd01",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image - LiveAction F16C12\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ec7828-1650-4742-a017-768f1e171847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 240, 320); 5.25159847402915\n",
      "Size: (1, 3, 1080, 1920); 12.283675133820502\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/rgb_f16c12_ft.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,3,240,320), (1,3,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(x)\n",
    "            latent = model.quantize.compand(z).round()\n",
    "        webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "        buff = io.BytesIO()\n",
    "        webp[0].save(buff, format='WEBP', lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820ea25-0e6b-4320-8eea-8118ce4696a7",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image - walloc 16x\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213c3e39-1ab3-422d-b1cd-03bd87df94a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 240, 320); 18.95507515593739\n",
      "Size: (1, 3, 1080, 1920); 32.51476243569805\n",
      "Size: (1, 3, 240, 320); 25.942059048079244\n",
      "Size: (1, 3, 1080, 1920); 134.043967301646\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from walloc import walloc\n",
    "from walloc.walloc import latent_to_pil, pil_to_latent\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "device = \"cpu\"\n",
    "config_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_16x.json\"\n",
    ")\n",
    "codec_config = SimpleNamespace(**json.load(open(config_file)))\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_16x.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "codec = walloc.Codec2D(\n",
    "    channels = codec_config.channels,\n",
    "    J = codec_config.J,\n",
    "    Ne = codec_config.Ne,\n",
    "    Nd = codec_config.Nd,\n",
    "    latent_dim = codec_config.latent_dim,\n",
    "    latent_bits = codec_config.latent_bits,\n",
    "    lightweight_encode = codec_config.lightweight_encode\n",
    ")\n",
    "codec.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "codec.eval();\n",
    "\n",
    "for (device, dtype)  in [(\"cpu\", torch.float32), (\"cuda\", torch.bfloat16)]:\n",
    "    codec = codec.to(device).to(dtype)\n",
    "\n",
    "    for S in [(1,3,240,320), (1,3,1080,1920)]:\n",
    "        encode_time = []\n",
    "        for i_trial in range(101):\n",
    "            x = torch.randn(S, device=device, dtype=dtype).clamp(-0.5,0.5)\n",
    "            t0 = time.time()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                z = codec.encoder(codec.wavelet_analysis(x, J=codec.J))\n",
    "            webp = latent_to_pil(z.cpu(), n_bits=8, C=3)\n",
    "            buff = io.BytesIO()\n",
    "            webp[0].save(buff, format='WEBP', lossless=True)\n",
    "            \n",
    "            encode_time.append(time.time() - t0)\n",
    "        print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319d7eb-0017-4fa5-bbf0-28762b3fd54c",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image – JPEG 2000\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d77bcfc-19a8-4cf4-b720-6560db0bb3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (3, 240, 320); 5.193123833066432\n",
      "Size: (3, 1080, 1920); 6.296993503691155\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "\n",
    "for S in [(3,240,320), (3,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "        buff = io.BytesIO()\n",
    "        img = to_pil_image(x)\n",
    "        img.save(buff, format= \"JPEG2000\", quality_layers=[24])\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0c2d3-25c3-4025-9821-4bfb4e13d7d9",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image - Balle 2018 (Q=1)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b3ee79-fc94-4a9d-b22c-2ebc5f566365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 256, 320); 3.4403086256683424\n",
      "Size: (1, 3, 1080, 1920); 5.105603504349618\n"
     ]
    }
   ],
   "source": [
    "import torch, compressai, time, PIL.Image, numpy as np, pickle, gzip\n",
    "model = compressai.zoo.bmshj2018_hyperprior(quality=1, metric='mse', pretrained=True)\n",
    "device = 'cpu'\n",
    "model.to(device);\n",
    "model.eval();\n",
    "for S in [(1,3,256,320), (1,3,1080,1920)]: # balle 2018 requires input to be multiple of 32\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            z = model.compress(x)\n",
    "        with gzip.open('temp.pkl.gz', \"wb\") as f:\n",
    "            pickle.dump(z, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5438e-28f6-49cf-888f-fda225541d06",
   "metadata": {},
   "source": [
    "---\n",
    "RGB Image - Balle 2018 (Q=8)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c19f7b-4e73-48d9-bc64-018084394c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://compressai.s3.amazonaws.com/models/v1/bmshj2018-hyperprior-8-a583f0cf.pth.tar\" to /home/dgj335/.cache/torch/hub/checkpoints/bmshj2018-hyperprior-8-a583f0cf.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46.0M/46.0M [00:09<00:00, 5.32MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 256, 320); 2.7300617659724926\n",
      "Size: (1, 3, 1080, 1920); 2.7881128010450644\n"
     ]
    }
   ],
   "source": [
    "import torch, compressai, time, PIL.Image, numpy as np, pickle, gzip\n",
    "model = compressai.zoo.bmshj2018_hyperprior(quality=8, metric='mse', pretrained=True)\n",
    "device = 'cpu'\n",
    "model.to(device);\n",
    "model.eval();\n",
    "for S in [(1,3,256,320), (1,3,1080,1920)]: # balle 2018 requires input to be multiple of 32\n",
    "    encode_time = []\n",
    "    for i_trial in range(101):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            z = model.compress(x)\n",
    "        with gzip.open('temp.pkl.gz', \"wb\") as f:\n",
    "            pickle.dump(z, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/3/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478b6a8-af02-4525-b2d4-643961dcd275",
   "metadata": {},
   "source": [
    "---\n",
    "Hyperspectral - JPEG 2000\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0780f0d1-199b-417f-b286-d31029a83760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 1, 224, 224, 224); 6.297860927809051\n",
      "Size: (1, 1, 224, 1024, 1024); 6.4482910390966905\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "\n",
    "for S in [(1,1,224,224,224), (1,1,224,1024,1024)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        img_list = [to_pil_image(x[0,:,i]) for i in range(224)]\n",
    "        buff = []\n",
    "        for img in img_list:\n",
    "            buff.append(io.BytesIO())\n",
    "            img.save(buff[-1], format= \"JPEG2000\", quality_layers=[284])\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153d531-e76b-45b1-bc89-1ae03f4f77fd",
   "metadata": {},
   "source": [
    "---\n",
    "Hyperspectral - LiveAction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e0800b-dbc7-4249-a036-0f79bb19b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 1, 224, 224, 224); 13.758094663558309\n",
      "Size: (1, 1, 224, 1024, 1024); 14.93040220445062\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/hyper_f8c8.pth', map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J=int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth=config.encoder_depth,\n",
    "    encoder_kernel_size=config.encoder_kernel_size,\n",
    "    decoder_depth=config.decoder_depth,\n",
    "    lightweight_encode=config.lightweight_encode,\n",
    "    lightweight_decode=config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,1,224,224,224), (1,1,224,1024,1024)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        z = einops.rearrange(z, 'b c f h w -> (c f) b h w')\n",
    "        img_list = latent_to_pil(z.cpu(), n_bits=8, C=1)\n",
    "        \n",
    "        buff = []\n",
    "        for img in img_list:\n",
    "            buff.append(io.BytesIO())\n",
    "            img.save(buff[-1], format= \"TIFF\", compression='tiff_adobe_deflate')\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539c0cb3-196a-4c5d-a19e-c9acddca7d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 192, 224, 224); 4.78225396588765\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/hyper_f8c8.pth', map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=192,\n",
    "    J=5,\n",
    "    latent_dim=192*8*8,\n",
    "    encoder_depth=4,\n",
    "    encoder_kernel_size=1,\n",
    "    decoder_depth=1,\n",
    "    lightweight_encode=True,\n",
    "    lightweight_decode=True,\n",
    ")\n",
    "del model.decoder_blocks\n",
    "model.to(device)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,192,224,224)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        img = latent_to_pil(z.cpu(), n_bits=8, C=3)[0]\n",
    "        \n",
    "        buff = io.BytesIO()\n",
    "        # img.save(buff, format= \"TIFF\", compression='tiff_adobe_deflate')\n",
    "        img.save(buff, format= \"WEBP\", lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51536225-dda9-401f-a23b-c35c5b626f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 192, 224, 224); 13.610920358944906\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/hyper_f8c8.pth', map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=192,\n",
    "    J=4,\n",
    "    latent_dim=192*4*4,\n",
    "    encoder_depth=4,\n",
    "    encoder_kernel_size=1,\n",
    "    decoder_depth=1,\n",
    "    lightweight_encode=True,\n",
    "    lightweight_decode=True,\n",
    ")\n",
    "del model.decoder_blocks\n",
    "model.to(device)\n",
    "model.eval();\n",
    "\n",
    "for S in [(1,192,224,224)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        img = latent_to_pil(z.cpu(), n_bits=8, C=3)[0]\n",
    "        \n",
    "        buff = io.BytesIO()\n",
    "        # img.save(buff, format= \"TIFF\", compression='tiff_adobe_deflate')\n",
    "        img.save(buff, format= \"WEBP\", lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3df67-93df-4928-a57e-156586bda70d",
   "metadata": {},
   "source": [
    "---\n",
    "Hyperspectral - WaLLoC\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5ec07c-3184-4d57-abfb-160d0e973d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 192, 224, 224); 27.785683464055452\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from walloc import walloc\n",
    "from walloc.walloc import latent_to_pil, pil_to_latent\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "config_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_16x.json\"\n",
    ")\n",
    "codec_config = SimpleNamespace(**json.load(open(config_file)))\n",
    "codec = walloc.Codec2D(\n",
    "    channels = 192,\n",
    "    J = 4,\n",
    "    Ne = None,\n",
    "    Nd = 768,\n",
    "    latent_dim = 192*4*4,\n",
    "    latent_bits = codec_config.latent_bits,\n",
    "    lightweight_encode = codec_config.lightweight_encode\n",
    ")\n",
    "\n",
    "codec.eval();\n",
    "\n",
    "for (device, dtype)  in [(\"cpu\", torch.float32)]:\n",
    "    codec = codec.to(device).to(dtype)\n",
    "\n",
    "    for S in [(1,192,224,224)]:\n",
    "        encode_time = []\n",
    "        for i_trial in range(13):\n",
    "            x = torch.randn(S, device=device, dtype=dtype).clamp(-0.5,0.5)\n",
    "            t0 = time.time()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                z = codec.encoder(codec.wavelet_analysis(x, J=codec.J))\n",
    "            webp = latent_to_pil(z.cpu(), n_bits=8, C=3)\n",
    "            buff = io.BytesIO()\n",
    "            webp[0].save(buff, format='WEBP', lossless=True)\n",
    "            \n",
    "            encode_time.append(time.time() - t0)\n",
    "        print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393c4e89-e4e7-4ef4-9673-83948d2a3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 192, 224, 224); 3.861904831247297\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from walloc import walloc\n",
    "from walloc.walloc import latent_to_pil, pil_to_latent\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "config_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/walloc\",\n",
    "    filename=\"RGB_16x.json\"\n",
    ")\n",
    "codec_config = SimpleNamespace(**json.load(open(config_file)))\n",
    "codec = walloc.Codec2D(\n",
    "    channels = 192,\n",
    "    J = 5,\n",
    "    Ne = None,\n",
    "    Nd = 768,\n",
    "    latent_dim = 192*8*8,\n",
    "    latent_bits = codec_config.latent_bits,\n",
    "    lightweight_encode = codec_config.lightweight_encode\n",
    ")\n",
    "\n",
    "codec.eval();\n",
    "\n",
    "for (device, dtype)  in [(\"cpu\", torch.float32)]:\n",
    "    codec = codec.to(device).to(dtype)\n",
    "\n",
    "    for S in [(1,192,224,224)]:\n",
    "        encode_time = []\n",
    "        for i_trial in range(13):\n",
    "            x = torch.randn(S, device=device, dtype=dtype).clamp(-0.5,0.5)\n",
    "            t0 = time.time()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                z = codec.encoder(codec.wavelet_analysis(x, J=codec.J))\n",
    "            webp = latent_to_pil(z.cpu(), n_bits=8, C=3)\n",
    "            buff = io.BytesIO()\n",
    "            webp[0].save(buff, format='WEBP', lossless=True)\n",
    "            \n",
    "            encode_time.append(time.time() - t0)\n",
    "        print(f'Size: {S}; {x.numel()/np.median(encode_time)/1e6}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f553b-ea18-483e-9290-7de2ebdbd0a0",
   "metadata": {},
   "source": [
    "---\n",
    "Video - LiveAction F8C48\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210e4093-b478-4302-92f2-2a23686c17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 24, 240, 320); 107.623524581751\n",
      "Size: (1, 3, 24, 1080, 1920); 2.386498597486046\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/video_f8c48.pth', map_location=\"cpu\",weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train();\n",
    "\n",
    "for S in [(1,3,24,240,320), (1,3,24,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        for chunk in latent_to_pil(einops.rearrange(z[0], 'c f h w -> f c h w').cpu(),n_bits=8,C=3):\n",
    "            buff = io.BytesIO()\n",
    "            chunk.save(buff,format='webp',lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {24/np.median(encode_time)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b380799-04ea-42c6-a2d1-5e930c4d2491",
   "metadata": {},
   "source": [
    "---\n",
    "Video - LiveAction F8C12\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0dd81d-752a-4d59-9bdf-491518b4e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (1, 3, 24, 240, 320); 108.71682063931951\n",
      "Size: (1, 3, 24, 1080, 1920); 2.399737787287733\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image\n",
    "import einops\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "\n",
    "device = \"cpu\"\n",
    "checkpoint = torch.load('../../hf/autocodec/video_f8c12.pth', map_location=\"cpu\",weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train();\n",
    "\n",
    "for S in [(1,3,24,240,320), (1,3,24,1080,1920)]:\n",
    "    encode_time = []\n",
    "    for i_trial in range(3):\n",
    "        x = torch.randn(S).clamp(-1,1)\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model.quantize.compand(model.encode(x)).round()\n",
    "        for chunk in latent_to_pil(einops.rearrange(z[0], 'c f h w -> f c h w').cpu(),n_bits=8,C=3):\n",
    "            buff = io.BytesIO()\n",
    "            chunk.save(buff,format='webp',lossless=True)\n",
    "        \n",
    "        encode_time.append(time.time() - t0)\n",
    "    print(f'Size: {S}; {24/np.median(encode_time)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
