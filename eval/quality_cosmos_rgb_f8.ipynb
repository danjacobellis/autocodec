{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc4d032-ac60-4670-9f79-f48f9a6ee4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from huggingface_hub import snapshot_download\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer\n",
    "from torchvision.transforms.v2 import Pad, CenterCrop\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bae16e-d171-4dbb-ba37-5672dc433186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b253d2ba47c641db9cb2f6c4ff3fca49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a98778b59e84f1f8e484910914d7e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4975f288fcb94902a31ec1970564e989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "ssim_loss = SSIMLoss().to(device)\n",
    "kodak = datasets.load_dataset(\"danjacobellis/kodak\", split='validation')\n",
    "lsdir = datasets.load_dataset(\"danjacobellis/LSDIR_val\", split='validation')\n",
    "inet = datasets.load_dataset(\"timm/imagenet-1k-wds\",split='validation')\n",
    "model_path = snapshot_download(repo_id='nvidia/Cosmos-Tokenizer-DI8x8')\n",
    "encoder = ImageTokenizer(checkpoint_enc=f'{model_path}/encoder.jit').to(device)\n",
    "decoder = ImageTokenizer(checkpoint_dec=f'{model_path}/decoder.jit').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f643d1d8-a23b-49bd-8a2d-cc74719cf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality_h1024(sample):\n",
    "    img = sample['jpg'].convert(\"RGB\")\n",
    "    aspect = img.width/img.height\n",
    "    img = img.resize((int(16*(1024*aspect//16)),1024),resample=PIL.Image.Resampling.LANCZOS)\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.float) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        z, _ = encoder.encode(x_orig)\n",
    "    encode_time = time.time() - t0\n",
    "    size_bytes = 2*z.numel()\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        z, _ = encoder.encode(x_orig)\n",
    "        x_hat = decoder.decode(z).to(torch.float).clamp(-1,1)\n",
    "    decode_time = time.time() - t0\n",
    "\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "\n",
    "    pixels = img.width * img.height\n",
    "    bpp = 8 * size_bytes / pixels\n",
    "    mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "    PSNR = -10 * mse.log10().item()\n",
    "    LPIPS_dB = -10 * np.log10(lpips_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    DISTS_dB = -10 * np.log10(dists_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item())\n",
    "    SSIM = 1 - ssim_loss(x_orig_01.to(\"cuda\"), x_hat_01.to(\"cuda\")).item()\n",
    "\n",
    "    return {\n",
    "        'encode_time': encode_time,\n",
    "        'decode_time': decode_time,\n",
    "        'bpp': bpp,\n",
    "        'PSNR': PSNR,\n",
    "        'LPIPS_dB': LPIPS_dB,\n",
    "        'DISTS_dB': DISTS_dB,\n",
    "        'SSIM': SSIM,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f8325c-2b49-47a5-aeaa-635baf18d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function evaluate_quality_h1024 at 0x786dac628430> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c652ca7213ab4addb46a8baf923c3540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_138396.py\", line 14, in forward\n    quant_conv = self.quant_conv\n    encoder = self.encoder\n    _0 = (quant_conv).forward((encoder).forward(input, ), )\n                               ~~~~~~~~~~~~~~~~ <--- HERE\n    _1, _2, _3, = (quantizer).forward(_0, )\n    return (_1, _2, _3)\n  File \"code/__torch__/projects/edify_tokenizer/v1/module/layers2d/___torch_mangle_138391.py\", line 55, in forward\n    _6 = (_00).forward((downsample).forward(_4, ), )\n    _3 = (_1).forward((_0).forward((_11).forward(_6, ), ), )\n    _7 = (attn_1).forward((block_1).forward(_3, ), )\n          ~~~~~~~~~~~~~~~ <--- HERE\n    _8 = (norm_out).forward((block_2).forward(_7, ), )\n    input0 = torch.mul(_8, torch.sigmoid(_8))\n  File \"code/__torch__/projects/edify_tokenizer/v1/module/layers2d/___torch_mangle_138380.py\", line 40, in forward\n    k0 = torch.reshape(_2, [_6, _10, int(torch.mul(h, w))])\n    w_ = torch.bmm(q1, k0)\n    input = torch.mul(w_, CONSTANTS.c2)\n            ~~~~~~~~~ <--- HERE\n    w_0 = torch.softmax(input, 2)\n    v0 = torch.reshape(_3, [_5, _9, int(torch.mul(h, w))])\n\nTraceback of TorchScript, original code (most recent call last):\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/module/layers2d.py(112): forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1542): _slow_forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1561): _call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1552): _wrapped_call_impl\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/module/layers2d.py(205): forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1542): _slow_forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1561): _call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1552): _wrapped_call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py(218): forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1542): _slow_forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1561): _call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1552): _wrapped_call_impl\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(1274): trace_module\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(694): _trace_impl\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(999): trace\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py(574): _fn\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/checkpointer.py(285): _get_ema_jit\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/checkpointer.py(96): save\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/trainer.py(152): train\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/scripts/train.py(45): launch\n/usr/local/lib/python3.10/dist-packages/loguru/_logger.py(1277): catch_wrapper\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/scripts/train.py(84): <module>\n/usr/lib/python3.10/runpy.py(86): _run_code\n/usr/lib/python3.10/runpy.py(196): _run_module_as_main\nRuntimeError: CUDA out of memory. Tried to allocate 14.03 GiB. GPU 0 has a total capacity of 23.65 GiB of which 7.67 GiB is free. Including non-PyTorch memory, this process has 15.94 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 974.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_dataset \u001b[38;5;241m=\u001b[39m \u001b[43minet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_quality_h1024\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3487\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3486\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3487\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3488\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3489\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3461\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3460\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3461\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3390\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3388\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3389\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3390\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mevaluate_quality_h1024\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      8\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m     z, _ \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m encode_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m     12\u001b[0m size_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mz\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cosmos_tokenizer/image_lib.py:108\u001b[0m, in \u001b[0;36mImageTokenizer.encode\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encodes an image into a latent embedding or code.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m            again (H/h x W/w), and channel dimension of 6.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     output_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enc_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_latent, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output_latent\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_138396.py\", line 14, in forward\n    quant_conv = self.quant_conv\n    encoder = self.encoder\n    _0 = (quant_conv).forward((encoder).forward(input, ), )\n                               ~~~~~~~~~~~~~~~~ <--- HERE\n    _1, _2, _3, = (quantizer).forward(_0, )\n    return (_1, _2, _3)\n  File \"code/__torch__/projects/edify_tokenizer/v1/module/layers2d/___torch_mangle_138391.py\", line 55, in forward\n    _6 = (_00).forward((downsample).forward(_4, ), )\n    _3 = (_1).forward((_0).forward((_11).forward(_6, ), ), )\n    _7 = (attn_1).forward((block_1).forward(_3, ), )\n          ~~~~~~~~~~~~~~~ <--- HERE\n    _8 = (norm_out).forward((block_2).forward(_7, ), )\n    input0 = torch.mul(_8, torch.sigmoid(_8))\n  File \"code/__torch__/projects/edify_tokenizer/v1/module/layers2d/___torch_mangle_138380.py\", line 40, in forward\n    k0 = torch.reshape(_2, [_6, _10, int(torch.mul(h, w))])\n    w_ = torch.bmm(q1, k0)\n    input = torch.mul(w_, CONSTANTS.c2)\n            ~~~~~~~~~ <--- HERE\n    w_0 = torch.softmax(input, 2)\n    v0 = torch.reshape(_3, [_5, _9, int(torch.mul(h, w))])\n\nTraceback of TorchScript, original code (most recent call last):\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/module/layers2d.py(112): forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1542): _slow_forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1561): _call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1552): _wrapped_call_impl\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/module/layers2d.py(205): forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1542): _slow_forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1561): _call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1552): _wrapped_call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py(218): forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1542): _slow_forward\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1561): _call_impl\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1552): _wrapped_call_impl\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(1274): trace_module\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(694): _trace_impl\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(999): trace\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py(574): _fn\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/checkpointer.py(285): _get_ema_jit\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/checkpointer.py(96): save\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/projects/edify_tokenizer/v1/trainer.py(152): train\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/scripts/train.py(45): launch\n/usr/local/lib/python3.10/dist-packages/loguru/_logger.py(1277): catch_wrapper\n/lustre/fsw/portfolios/nvr/projects/nvr_picasso/freda/projects/edify_tokenizer1/cosmos/DI1024_FSQ_cosmos_8x8_1118b_adv/scripts/train.py(84): <module>\n/usr/lib/python3.10/runpy.py(86): _run_code\n/usr/lib/python3.10/runpy.py(196): _run_module_as_main\nRuntimeError: CUDA out of memory. Tried to allocate 14.03 GiB. GPU 0 has a total capacity of 23.65 GiB of which 7.67 GiB is free. Including non-PyTorch memory, this process has 15.94 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 974.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "results_dataset = inet.map(evaluate_quality_h1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4c01d-d8fe-4d99-9f99-080619403441",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in [\n",
    "    'bpp',\n",
    "    'PSNR',\n",
    "    'LPIPS_dB',\n",
    "    'DISTS_dB',\n",
    "    'SSIM',\n",
    "]:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
