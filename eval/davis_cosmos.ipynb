{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from timm.optim import Mars\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import HTML\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import CenterCrop, RandomCrop\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from decord import VideoReader\n",
    "from huggingface_hub import snapshot_download\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550eece5-fccd-4fba-ba81-f81f73a3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/davis\").cast_column('video',datasets.Video()).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212dc070-e81c-46ae-a394-b476d4010a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d39c24ec4049a68493dfe85b0f7f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = snapshot_download(repo_id='nvidia/Cosmos-Tokenizer-DV8x8x8')\n",
    "encoder = ImageTokenizer(checkpoint_enc=f'{model_path}/encoder.jit').to(device)\n",
    "decoder = ImageTokenizer(checkpoint_dec=f'{model_path}/decoder.jit').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb2bf0e-b731-4c11-9201-ac26cbf1a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad3d(x, p, extra, small_dim_mode):\n",
    "    b, c, f, h, w = x.shape\n",
    "    extra_f, extra_h, extra_w = extra  # Unpack the extra tuple for each dimension\n",
    "    \n",
    "    for dim, size, extra_pad in zip(['f', 'h', 'w'], [f, h, w], [extra_f, extra_h, extra_w]):\n",
    "        if small_dim_mode and size < p:\n",
    "            pad1 = extra_pad\n",
    "            pad2 = extra_pad\n",
    "        else:\n",
    "            t = math.ceil(size / p) * p\n",
    "            pad_total = t - size\n",
    "            pad1 = pad_total // 2\n",
    "            pad2 = pad_total - pad1\n",
    "            pad1 += extra_pad\n",
    "            pad2 += extra_pad\n",
    "        if dim == 'f':\n",
    "            fp1, fp2 = pad1, pad2\n",
    "        elif dim == 'h':\n",
    "            hp1, hp2 = pad1, pad2\n",
    "        elif dim == 'w':\n",
    "            wp1, wp2 = pad1, pad2\n",
    "            \n",
    "    return torch.nn.functional.pad(\n",
    "        x,\n",
    "        pad=(wp1, wp2, hp1, hp2, fp1, fp2),\n",
    "        mode=\"reflect\"\n",
    "    )\n",
    "\n",
    "def center_crop_3d(x, f, h, w):\n",
    "    assert x.ndim == 5\n",
    "    _, _, F, H, W = x.shape\n",
    "    front = (F - f) // 2\n",
    "    back  = front + f\n",
    "    top   = (H - h) // 2\n",
    "    bottom = top + h\n",
    "    left  = (W - w) // 2\n",
    "    right = left + w\n",
    "    return x[:, :, front:back, top:bottom, left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24b72a7-8f04-40ec-8556-de62e892d93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='90' class='' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [90/90 33:53&lt;00:00 PSNR: 26.9914060499933, CR:192.0]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F = 8\n",
    "target_w = 1920\n",
    "target_h = 1080\n",
    "PSNR_list = []\n",
    "CR_list = []\n",
    "pb = progress_bar(dataset['train'])\n",
    "encode_time = 0\n",
    "decode_time = 0\n",
    "total_frames = 0\n",
    "for sample in pb:\n",
    "    \n",
    "    video = sample['video']\n",
    "    len_video = len(video)\n",
    "    total_frames += len_video\n",
    "    xr = video.get_batch(range(len_video))\n",
    "    xr = einops.rearrange(xr, 'f h w c -> c f h w')\n",
    "    x = []\n",
    "    for i_frame in range(len_video):\n",
    "        frame = xr[:, i_frame]\n",
    "        pil_img = to_pil_image(frame)\n",
    "        resized_img = pil_img.resize((target_w, target_h))\n",
    "        tensor_frame = pil_to_tensor(resized_img).unsqueeze(1)\n",
    "        x.append(tensor_frame)\n",
    "    x = torch.cat(x, dim=1).unsqueeze(0)\n",
    "    x = x / 127.5 - 1.0\n",
    "    x_orig = x.clone()\n",
    "    x = pad3d(x, p=F, extra=(8,0,0), small_dim_mode=True)\n",
    "\n",
    "    x_hat = []\n",
    "    for i_chunk in range(x.shape[2]//F - 2):\n",
    "        f1 = i_chunk * F\n",
    "        f2 = f1 + 3*F\n",
    "        x_chunk = x[:, :, f1:f2, :, :].to(torch.bfloat16).to(device)\n",
    "        with torch.no_grad():\n",
    "            t0 = time.time()\n",
    "            z = encoder.encode(x_chunk)[0]\n",
    "            encode_time += time.time()-t0\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            t0 = time.time()\n",
    "            x_hat.append(decoder.decode(z).clamp(-1,1)[:,:,8:16].detach().cpu())\n",
    "            decode_time += time.time() - t0\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    x_hat = torch.cat(x_hat,dim=2).to(torch.float)\n",
    "    \n",
    "    _,_,f,h,w = x_orig.shape\n",
    "    x_hat = center_crop_3d(x_hat,f,h,w)\n",
    "    \n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    PSNR = []\n",
    "    for i_frame in range(x_orig_01.shape[2]):\n",
    "        mse = torch.nn.functional.mse_loss(x_orig_01[0, :, i_frame], x_hat_01[0, :, i_frame])\n",
    "        PSNR.append(-10 * mse.log10().item())\n",
    "    PSNR_list.append(PSNR)\n",
    "    \n",
    "    size_bytes = 2*z.numel()*(x.shape[2]//F - 2)\n",
    "    CR_list.append(x_orig.numel()/size_bytes)\n",
    "\n",
    "    pb.comment = (f\"PSNR: {np.mean(PSNR)}, CR:{CR_list[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6d0c58-423e-4e23-8012-f43d0b4bb395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(20.359501838684082)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([np.min(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9805fc32-2dd1-49cc-8a36-f211376a2e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(27.427838981947403)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3396bfc8-b269-4877-897e-25dcd0ee9a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(41.94117546081543)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([np.max(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c728e156-e28a-4cb6-bbfc-3ad2d4fee172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.7328405095795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_frames/encode_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d4212c-295f-4afc-a5d9-bf2fd8a243cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.226943773470326"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_frames/decode_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39b68c5-472b-4542-9ab6-a07dc0c5848f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.47641808066405"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_h*target_w*(total_frames/encode_time)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a51728-0d39-49e8-9771-274187980fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.05939060866807"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_h*target_w*(total_frames/decode_time)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b121600a-cd7c-42f1-8234-66c9744f793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(182.43030303030304)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(CR_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
