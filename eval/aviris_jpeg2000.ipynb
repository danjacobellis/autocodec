{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b4dc22-bdb4-4327-8964-869e0631a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import datasets\n",
    "import einops\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c7672d-c57e-42b5-9c96-fff6fa841f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dc549537294faa961ed4c0f8cf9983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219e5cde229747b89fd0408ebbe3207c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c5940edf2644fb8651caf06c9e55a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "aviris = datasets.load_dataset(\"danjacobellis/aviris_1k_val\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf665d4-12d9-461c-9ea2-e3ce652004ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad3d(x, p, extra, small_dim_mode):\n",
    "    b, c, f, h, w = x.shape\n",
    "    extra_f, extra_h, extra_w = extra  # Unpack the extra tuple for each dimension\n",
    "    \n",
    "    for dim, size, extra_pad in zip(['f', 'h', 'w'], [f, h, w], [extra_f, extra_h, extra_w]):\n",
    "        if small_dim_mode and size < p:\n",
    "            pad1 = extra_pad\n",
    "            pad2 = extra_pad\n",
    "        else:\n",
    "            t = math.ceil(size / p) * p\n",
    "            pad_total = t - size\n",
    "            pad1 = pad_total // 2\n",
    "            pad2 = pad_total - pad1\n",
    "            pad1 += extra_pad\n",
    "            pad2 += extra_pad\n",
    "        if dim == 'f':\n",
    "            fp1, fp2 = pad1, pad2\n",
    "        elif dim == 'h':\n",
    "            hp1, hp2 = pad1, pad2\n",
    "        elif dim == 'w':\n",
    "            wp1, wp2 = pad1, pad2\n",
    "            \n",
    "    return torch.nn.functional.pad(\n",
    "        x,\n",
    "        pad=(wp1, wp2, hp1, hp2, fp1, fp2),\n",
    "        mode=\"reflect\"\n",
    "    )\n",
    "\n",
    "def center_crop_3d(x, f, h, w):\n",
    "    assert x.ndim == 5\n",
    "    _, _, F, H, W = x.shape\n",
    "    front = (F - f) // 2\n",
    "    back  = front + f\n",
    "    top   = (H - h) // 2\n",
    "    bottom = top + h\n",
    "    left  = (W - w) // 2\n",
    "    right = left + w\n",
    "    return x[:, :, front:back, top:bottom, left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54e8d8a5-1a14-4bb1-b9ee-c0543bea5d75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m xhat = torch.cat([pil_to_tensor(PIL.Image.open(b)) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m buff])\n\u001b[32m     23\u001b[39m decode_time = time.time() - t0\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[32m1\u001b[39m==\u001b[32m0\u001b[39m\n\u001b[32m     26\u001b[39m xhat = center_crop_3d(x=xhat, f=x_orig.shape[\u001b[32m2\u001b[39m], h=x_orig.shape[\u001b[32m3\u001b[39m], w=x_orig.shape[\u001b[32m4\u001b[39m])\n\u001b[32m     27\u001b[39m mse = torch.nn.functional.mse_loss(x_orig.to(torch.float),xhat.to(torch.float))\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sample = aviris[0]\n",
    "img = sample['image']\n",
    "bands = []\n",
    "for i in range(img.n_frames):\n",
    "    img.seek(i)\n",
    "    bands.append(np.array(img, dtype=np.int16))\n",
    "x_orig = torch.from_numpy(np.stack(bands)).to(device).to(torch.float16).unsqueeze(0).unsqueeze(0) / (2**16) + 0.5\n",
    "x = pad3d(x_orig, 8, extra=(0,0,0), small_dim_mode=False)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "buff = []\n",
    "for img in img_list:\n",
    "    buff.append(io.BytesIO())\n",
    "    img.save(buff[-1], format= \"JPEG2000\", quality_layers=[550])\n",
    "encode_time = time.time() - t0\n",
    "\n",
    "size_bytes = sum(len(b.getbuffer()) for b in buff)\n",
    "cr = 2*x.numel() / size_bytes\n",
    "\n",
    "t0 = time.time()\n",
    "xhat = torch.cat([pil_to_tensor(PIL.Image.open(b)) for b in buff])\n",
    "decode_time = time.time() - t0\n",
    "\n",
    "assert 1==0\n",
    "xhat = center_crop_3d(x=xhat.unsqueeze(0).unsqueeze(0), f=x_orig.shape[2], h=x_orig.shape[3], w=x_orig.shape[4])\n",
    "mse = torch.nn.functional.mse_loss(x_orig.to(torch.float),xhat.to(torch.float))\n",
    "psnr = -10*mse.log10().item() + 6.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1814b9ab-8bcd-4230-a461-13418189254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = torch.cat([pil_to_tensor(PIL.Image.open(b)) for b in buff])\n",
    "\n",
    "xhat = center_crop_3d(x=xhat.unsqueeze(0).unsqueeze(0), f=x_orig.shape[2], h=x_orig.shape[3], w=x_orig.shape[4])\n",
    "mse = torch.nn.functional.mse_loss(x_orig,xhat)\n",
    "psnr = -10*mse.log10().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a86209c-3015-484c-a7d9-203cc5e2f5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-42.578125"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8677d531-b5cc-409d-ab08-498aa0fc354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):   \n",
    "    img = sample['image']\n",
    "    bands = []\n",
    "    for i in range(img.n_frames):\n",
    "        img.seek(i)\n",
    "        bands.append(np.array(img, dtype=np.int16))\n",
    "    x_orig = torch.from_numpy(np.stack(bands)).to(device).to(torch.bfloat16).unsqueeze(0).unsqueeze(0) / 32768.0\n",
    "    x = pad3d(x_orig, config.F, extra=(0,0,0), small_dim_mode=False)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        z = model.quantize.compand(model.encode(x)).round()\n",
    "    z = einops.rearrange(z, 'b c f h w -> (c f) b h w')\n",
    "    img_list = latent_to_pil(z.cpu(), n_bits=8, C=1)\n",
    "    \n",
    "    buff = []\n",
    "    for img in img_list:\n",
    "        buff.append(io.BytesIO())\n",
    "        img.save(buff[-1], format= \"TIFF\", compression='tiff_adobe_deflate')\n",
    "    encode_time = time.time() - t0\n",
    "    \n",
    "    size_bytes = sum(len(b.getbuffer()) for b in buff)\n",
    "    cr = 2*x.numel() / size_bytes\n",
    "    \n",
    "    t0 = time.time()\n",
    "    z = pil_to_latent([PIL.Image.open(b) for b in buff], N=1, n_bits=8, C=1)\n",
    "    z = einops.rearrange(z, '(c f) b h w -> b c f h w', c = config.latent_dim).to(device).to(torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        xhat = model.decode(z).clamp(-1,1)\n",
    "    \n",
    "    decode_time = time.time() - t0\n",
    "    \n",
    "    xhat = center_crop_3d(x=xhat, f=x_orig.shape[2], h=x_orig.shape[3], w=x_orig.shape[4])\n",
    "    mse = torch.nn.functional.mse_loss(x_orig.to(torch.float),xhat.to(torch.float))\n",
    "    psnr = -10*mse.log10().item() + 6.02\n",
    "\n",
    "    return {\n",
    "        \"voxels\": x_orig.numel(),\n",
    "        \"encode_time\": encode_time,\n",
    "        \"decode_time\": decode_time,\n",
    "        \"cr\": cr,\n",
    "        \"psnr\": psnr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235c4f55-0ba5-411a-add1-eeb92a7fb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'voxels',\n",
    "    'encode_time',\n",
    "    'decode_time',\n",
    "    'cr',\n",
    "    'psnr',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb4cfb7-ace4-447a-87f7-f18b302e77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset = aviris.map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e75f8a-05cb-4f13-8cf9-712dadbbb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "voxels: 87734420.76595744\n",
      "encode_time: 0.14616816094581117\n",
      "decode_time: 0.0333839883195593\n",
      "cr: 574.5738471332235\n",
      "psnr: 18.52037821049386\n",
      "600.1471162177203 MVox/sec\n",
      "2680.8554334029 MVox/sec\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in metrics:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")\n",
    "print(f\"{np.mean(np.array(results_dataset['voxels'])/1e6/np.array(results_dataset['encode_time']))} MVox/sec\")\n",
    "print(f\"{np.mean(np.array(results_dataset['voxels'])/1e6/np.array(results_dataset['decode_time']))} MVox/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
