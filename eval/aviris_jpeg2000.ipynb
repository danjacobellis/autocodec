{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b4dc22-bdb4-4327-8964-869e0631a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import datasets\n",
    "import einops\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c7672d-c57e-42b5-9c96-fff6fa841f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb7f8bec9d94eb58834433be1940ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d316f6d9c7744d9b8a93aa48c9218ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08241d778c4423d9bdf832c9db85ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "aviris = datasets.load_dataset(\"danjacobellis/aviris_1k_val\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf665d4-12d9-461c-9ea2-e3ce652004ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad3d(x, p, extra, small_dim_mode):\n",
    "    b, c, f, h, w = x.shape\n",
    "    extra_f, extra_h, extra_w = extra  # Unpack the extra tuple for each dimension\n",
    "    \n",
    "    for dim, size, extra_pad in zip(['f', 'h', 'w'], [f, h, w], [extra_f, extra_h, extra_w]):\n",
    "        if small_dim_mode and size < p:\n",
    "            pad1 = extra_pad\n",
    "            pad2 = extra_pad\n",
    "        else:\n",
    "            t = math.ceil(size / p) * p\n",
    "            pad_total = t - size\n",
    "            pad1 = pad_total // 2\n",
    "            pad2 = pad_total - pad1\n",
    "            pad1 += extra_pad\n",
    "            pad2 += extra_pad\n",
    "        if dim == 'f':\n",
    "            fp1, fp2 = pad1, pad2\n",
    "        elif dim == 'h':\n",
    "            hp1, hp2 = pad1, pad2\n",
    "        elif dim == 'w':\n",
    "            wp1, wp2 = pad1, pad2\n",
    "            \n",
    "    return torch.nn.functional.pad(\n",
    "        x,\n",
    "        pad=(wp1, wp2, hp1, hp2, fp1, fp2),\n",
    "        mode=\"reflect\"\n",
    "    )\n",
    "\n",
    "def center_crop_3d(x, f, h, w):\n",
    "    assert x.ndim == 5\n",
    "    _, _, F, H, W = x.shape\n",
    "    front = (F - f) // 2\n",
    "    back  = front + f\n",
    "    top   = (H - h) // 2\n",
    "    bottom = top + h\n",
    "    left  = (W - w) // 2\n",
    "    right = left + w\n",
    "    return x[:, :, front:back, top:bottom, left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8677d531-b5cc-409d-ab08-498aa0fc354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(sample):   \n",
    "    img = sample['image']\n",
    "    bands = []\n",
    "    for i in range(img.n_frames):\n",
    "        img.seek(i)\n",
    "        bands.append(np.array(img, dtype=np.int16))\n",
    "    x_orig = torch.from_numpy(np.stack(bands)).to(device).to(torch.float16).unsqueeze(0).unsqueeze(0) / (2**16) + 0.5\n",
    "    x = pad3d(x_orig, 8, extra=(0,0,0), small_dim_mode=False)\n",
    "\n",
    "    t0 = time.time()\n",
    "    img_list = [to_pil_image(x[0,:,i]) for i in range(224)]\n",
    "    buff = []\n",
    "    for img in img_list:\n",
    "        buff.append(io.BytesIO())\n",
    "        img.save(buff[-1], format= \"JPEG2000\", quality_layers=[284])\n",
    "    encode_time = time.time() - t0\n",
    "    \n",
    "    size_bytes = sum(len(b.getbuffer()) for b in buff)\n",
    "    cr = 2*x.numel() / size_bytes\n",
    "    \n",
    "    t0 = time.time()\n",
    "    xhat = torch.cat([pil_to_tensor(PIL.Image.open(b)) for b in buff])\n",
    "    decode_time = time.time() - t0\n",
    "    \n",
    "    xhat = center_crop_3d(x=xhat.unsqueeze(0).unsqueeze(0), f=x_orig.shape[2], h=x_orig.shape[3], w=x_orig.shape[4])\n",
    "    mse = torch.nn.functional.mse_loss(x_orig,xhat.to(torch.float)/255)\n",
    "    psnr = -10*mse.log10().item()\n",
    "\n",
    "    return {\n",
    "        \"voxels\": x_orig.numel(),\n",
    "        \"encode_time\": encode_time,\n",
    "        \"decode_time\": decode_time,\n",
    "        \"cr\": cr,\n",
    "        \"psnr\": psnr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235c4f55-0ba5-411a-add1-eeb92a7fb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'voxels',\n",
    "    'encode_time',\n",
    "    'decode_time',\n",
    "    'cr',\n",
    "    'psnr',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4cfb7-ace4-447a-87f7-f18b302e77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c847fcc9706408f89ddfca52d1071dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dataset = aviris.map(evaluate_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e75f8a-05cb-4f13-8cf9-712dadbbb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean\\n---\")\n",
    "for metric in metrics:\n",
    "    μ = np.mean(results_dataset[metric])\n",
    "    print(f\"{metric}: {μ}\")\n",
    "print(f\"{np.mean(np.array(results_dataset['voxels'])/1e6/np.array(results_dataset['encode_time']))} MVox/sec\")\n",
    "print(f\"{np.mean(np.array(results_dataset['voxels'])/1e6/np.array(results_dataset['decode_time']))} MVox/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
