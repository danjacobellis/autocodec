{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b138243-5dd9-4ac0-8ea1-5461bcb85e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "import random\n",
    "from timm.optim import Mars\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import HTML\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import CenterCrop, RandomCrop\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from decord import VideoReader\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550eece5-fccd-4fba-ba81-f81f73a3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/davis\").cast_column('video',datasets.Video()).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212dc070-e81c-46ae-a394-b476d4010a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ce6a09-9523-4724-a230-480333b4b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 2\n",
    "stride = 5\n",
    "block_size = stride + 2 * overlap\n",
    "target_w = 1920\n",
    "target_h = 1080\n",
    "upsampling_factor = config.F\n",
    "overlap_pixels = overlap * upsampling_factor  # e.g., 4 * 8 = 32\n",
    "central_size_pixels = stride * upsampling_factor  # e.g., 16 * 8 = 128\n",
    "\n",
    "def pad3d(x, p, extra, small_dim_mode):\n",
    "    b, c, f, h, w = x.shape\n",
    "    for dim, size in zip(['f', 'h', 'w'], [f, h, w]):\n",
    "        if small_dim_mode and size < p:\n",
    "            pad1 = extra\n",
    "            pad2 = extra\n",
    "        else:\n",
    "            t = math.ceil(size / p) * p\n",
    "            pad_total = t - size\n",
    "            pad1 = pad_total // 2\n",
    "            pad2 = pad_total - pad1\n",
    "            pad1 += extra\n",
    "            pad2 += extra\n",
    "        if dim == 'f':\n",
    "            fp1, fp2 = pad1, pad2\n",
    "        elif dim == 'h':\n",
    "            hp1, hp2 = pad1, pad2\n",
    "        elif dim == 'w':\n",
    "            wp1, wp2 = pad1, pad2\n",
    "    return torch.nn.functional.pad(\n",
    "        x,\n",
    "        pad=(wp1, wp2, hp1, hp2, fp1, fp2),\n",
    "        mode=\"reflect\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b72a7-8f04-40ec-8556-de62e892d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PSNR_list = []\n",
    "pb = progress_bar(dataset['train'])\n",
    "for sample in pb:\n",
    "    video = sample['video']\n",
    "    len_video = len(video)\n",
    "    xr = video.get_batch(range(len_video))\n",
    "    xr = einops.rearrange(xr, 'f h w c -> c f h w')\n",
    "    x = []\n",
    "    for i_frame in range(xr.shape[1]):\n",
    "        frame = xr[:, i_frame]\n",
    "        pil_img = to_pil_image(frame)\n",
    "        resized_img = pil_img.resize((target_w, target_h))\n",
    "        tensor_frame = pil_to_tensor(resized_img).unsqueeze(1)\n",
    "        x.append(tensor_frame)\n",
    "    x = torch.cat(x, dim=1).unsqueeze(0)\n",
    "    x = x / 127.5 - 1.0\n",
    "    x = x.to(device)\n",
    "    x_orig = x.clone()\n",
    "    x = pad3d(x, p=config.F*stride, extra=config.F*overlap, small_dim_mode=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x)\n",
    "        latent = model.quantize.compand(z).round()\n",
    "\n",
    "    # Latent dimensions\n",
    "    T_l, H_l, W_l = latent.shape[2], latent.shape[3], latent.shape[4]\n",
    "    F_padded = T_l * config.F\n",
    "    H_padded = H_l * config.F\n",
    "    W_padded = W_l * config.F\n",
    "    \n",
    "    # Initialize output tensor\n",
    "    x_hat = torch.zeros(1, 3, F_padded, H_padded, W_padded, device=device)\n",
    "    \n",
    "    # Number of blocks (ensure at least 1 block per dimension)\n",
    "    num_blocks_t = max(((T_l - block_size) // stride) + 1, 1)\n",
    "    num_blocks_h = max(((H_l - block_size) // stride) + 1, 1)\n",
    "    num_blocks_w = max(((W_l - block_size) // stride) + 1, 1)\n",
    "    \n",
    "    for k_t in range(num_blocks_t):\n",
    "        for k_h in range(num_blocks_h):\n",
    "            for k_w in range(num_blocks_w):\n",
    "                # Compute start indices (0 if only one block, else k * stride)\n",
    "                start_t = k_t * stride if num_blocks_t > 1 else 0\n",
    "                start_h = k_h * stride if num_blocks_h > 1 else 0\n",
    "                start_w = k_w * stride if num_blocks_w > 1 else 0\n",
    "                \n",
    "                # Compute end indices, clamped to latent dimensions\n",
    "                end_t = min(start_t + block_size, T_l)\n",
    "                end_h = min(start_h + block_size, H_l)\n",
    "                end_w = min(start_w + block_size, W_l)\n",
    "                \n",
    "                # Extract latent block\n",
    "                latent_block = latent[:, :, start_t:end_t, start_h:end_h, start_w:end_w]\n",
    "                \n",
    "                # Compute padding needed to reach block_size\n",
    "                pad_t_left = (block_size - (end_t - start_t)) // 2 if end_t - start_t < block_size else 0\n",
    "                pad_t_right = block_size - (end_t - start_t) - pad_t_left if end_t - start_t < block_size else 0\n",
    "                pad_h_left = (block_size - (end_h - start_h)) // 2 if end_h - start_h < block_size else 0\n",
    "                pad_h_right = block_size - (end_h - start_h) - pad_h_left if end_h - start_h < block_size else 0\n",
    "                pad_w_left = (block_size - (end_w - start_w)) // 2 if end_w - start_w < block_size else 0\n",
    "                pad_w_right = block_size - (end_w - start_w) - pad_w_left if end_w - start_w < block_size else 0\n",
    "                \n",
    "                # Pad the latent block if necessary\n",
    "                if any([pad_t_left, pad_t_right, pad_h_left, pad_h_right, pad_w_left, pad_w_right]):\n",
    "                    latent_block = torch.nn.functional.pad(\n",
    "                        latent_block,\n",
    "                        pad=(pad_w_left, pad_w_right, pad_h_left, pad_h_right, pad_t_left, pad_t_right),\n",
    "                        mode=\"reflect\"\n",
    "                    )\n",
    "                \n",
    "                # Decode the block\n",
    "                with torch.no_grad():\n",
    "                    x_hat_block = model.decode(latent_block).clamp(-1, 1)\n",
    "                \n",
    "                # Define central part and output placement\n",
    "                for dim, num_blocks, start, end, pad_left in [\n",
    "                    ('t', num_blocks_t, start_t, end_t, pad_t_left),\n",
    "                    ('h', num_blocks_h, start_h, end_h, pad_h_left),\n",
    "                    ('w', num_blocks_w, start_w, end_w, pad_w_left)\n",
    "                ]:\n",
    "                    if num_blocks > 1:\n",
    "                        # Overlapping blocks: take central stride portion\n",
    "                        start_central = overlap * upsampling_factor\n",
    "                        end_central = start_central + stride * upsampling_factor\n",
    "                        start_output = (start + overlap) * upsampling_factor\n",
    "                        end_output = start_output + stride * upsampling_factor\n",
    "                    else:\n",
    "                        # Single block: take middle portion corresponding to original size\n",
    "                        size = end - start  # Original size before padding\n",
    "                        start_central = pad_left * upsampling_factor\n",
    "                        end_central = start_central + size * upsampling_factor\n",
    "                        start_output = 0\n",
    "                        end_output = size * upsampling_factor\n",
    "                    \n",
    "                    if dim == 't':\n",
    "                        t_start_central, t_end_central = start_central, end_central\n",
    "                        t_pixel_start, t_pixel_end = start_output, end_output\n",
    "                    elif dim == 'h':\n",
    "                        h_start_central, h_end_central = start_central, end_central\n",
    "                        h_pixel_start, h_pixel_end = start_output, end_output\n",
    "                    elif dim == 'w':\n",
    "                        w_start_central, w_end_central = start_central, end_central\n",
    "                        w_pixel_start, w_pixel_end = start_output, end_output\n",
    "                \n",
    "                # Extract central part and place in output\n",
    "                central_part = x_hat_block[:, :,\n",
    "                                           t_start_central:t_end_central,\n",
    "                                           h_start_central:h_end_central,\n",
    "                                           w_start_central:w_end_central]\n",
    "                x_hat[:, :,\n",
    "                      t_pixel_start:t_pixel_end,\n",
    "                      h_pixel_start:h_pixel_end,\n",
    "                      w_pixel_start:w_pixel_end] = central_part\n",
    "    \n",
    "    # Crop to original dimensions\n",
    "    f_start = (F_padded - len_video) // 2\n",
    "    h_start = (H_padded - target_h) // 2\n",
    "    w_start = (W_padded - target_w) // 2\n",
    "    x_hat = x_hat[:, :,\n",
    "                  f_start:f_start + len_video,\n",
    "                  h_start:h_start + target_h,\n",
    "                  w_start:w_start + target_w]\n",
    "    x_hat = x_hat.clamp(-1, 1)\n",
    "    \n",
    "\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    PSNR = []\n",
    "    for i_frame in range(x_orig_01.shape[2]):\n",
    "        mse = torch.nn.functional.mse_loss(x_orig_01[0, :, i_frame], x_hat_01[0, :, i_frame])\n",
    "        PSNR.append(-10 * mse.log10().item())\n",
    "    PSNR_list.append(PSNR)\n",
    "\n",
    "    pb.comment = (f\"PSNR: {np.mean(PSNR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da8f9840-a858-4854-a7be-a2698383fc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='90' class='' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [90/90 1:46:58&lt;00:00 PSNR: 26.332859926753574]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(pb.progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf4e812d-c78b-435c-9118-cd0ef786fbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.441421025997553"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e927c66-b7ae-4413-b553-40c18e8f3b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.02947473526001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([np.max(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3e9f3a-52cb-4eb2-9c35-04cef6b00824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.028461456298828"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([np.min(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
