{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b138243-5dd9-4ac0-8ea1-5461bcb85e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "import random\n",
    "from timm.optim import Mars\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import HTML\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import CenterCrop, RandomCrop\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from decord import VideoReader\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550eece5-fccd-4fba-ba81-f81f73a3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/davis\").cast_column('video',datasets.Video()).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212dc070-e81c-46ae-a394-b476d4010a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../../hf/dance/video_f8c24.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89036aad-adbc-41f1-88cc-045bf9aa2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad3d(x,p=8):\n",
    "    b, c, f, h, w = x.shape\n",
    "    t = math.ceil(f / p) * p\n",
    "    fp1 = (t - f) // 2\n",
    "    fp2 = (t - f) - fp1\n",
    "    t = math.ceil(h / p) * p\n",
    "    hp1 = (t - h) // 2\n",
    "    hp2 = (t - h) - hp1\n",
    "    t = math.ceil(w / p) * p\n",
    "    wp1 = (t - w) // 2\n",
    "    wp2 = (t - w) - wp1\n",
    "    return torch.nn.functional.pad(x, pad=(wp1, wp2, hp1, hp2, fp1, fp2), mode=\"reflect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c62b49-b7c0-48e4-9877-b82930500339",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset['train'][1]\n",
    "video = sample['video']\n",
    "len_video = len(video)\n",
    "x1080 = video.get_batch(range(len_video))\n",
    "x1080 = einops.rearrange(x1080,'f h w c -> c f h w')\n",
    "x = []\n",
    "for i_frame in range(x1080.shape[1]):\n",
    "    frame = x1080[:,i_frame]\n",
    "    x.append(pil_to_tensor(to_pil_image(frame).resize((1920,1080))).unsqueeze(1))\n",
    "x = torch.cat(x,dim=1).unsqueeze(0)\n",
    "x = x/127.5 - 1.0\n",
    "x = x.to(device)\n",
    "x = pad3d(x)\n",
    "with torch.no_grad():\n",
    "    z = model.encode(x)\n",
    "    latent = model.quantize.compand(z).round().cpu()\n",
    "\n",
    "decode_bs = 6\n",
    "x_hat_list = []\n",
    "with torch.no_grad():\n",
    "    for start_idx in range(0, latent.shape[2], decode_bs):\n",
    "        end_idx = min(start_idx + decode_bs, latent.shape[2])\n",
    "        latent_batch = latent[:, :, range(start_idx,end_idx)].to(device)\n",
    "        x_hat_batch = model.decode(latent_batch).clamp(-1, 1)\n",
    "        x_hat_list.append(x_hat_batch)\n",
    "    x_hat = torch.cat(x_hat_list, dim=2)\n",
    "\n",
    "x_orig_01 = x / 2 + 0.5\n",
    "x_hat_01 = x_hat / 2 + 0.5\n",
    "\n",
    "mse = torch.nn.functional.mse_loss(x_orig_01[0], x_hat_01[0])\n",
    "PSNR = -10 * mse.log10().item()\n",
    "PSNR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
