{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac028e-ffa0-4ed2-9b2a-1ec0a4c1e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/danjacobellis/dance/resolve/main/video_f8c48_24f.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from timm.optim import Mars\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import HTML\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import CenterCrop, RandomCrop\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from decord import VideoReader\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550eece5-fccd-4fba-ba81-f81f73a3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/davis\").cast_column('video',datasets.Video()).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "212dc070-e81c-46ae-a394-b476d4010a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint_f8c48_784.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdb2bf0e-b731-4c11-9201-ac26cbf1a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_w = 1920\n",
    "target_h = 1080\n",
    "n_frames = 24\n",
    "\n",
    "def pad3d(x, p, extra, small_dim_mode):\n",
    "    b, c, f, h, w = x.shape\n",
    "    for dim, size in zip(['f', 'h', 'w'], [f, h, w]):\n",
    "        if small_dim_mode and size < p:\n",
    "            pad1 = extra\n",
    "            pad2 = extra\n",
    "        else:\n",
    "            t = math.ceil(size / p) * p\n",
    "            pad_total = t - size\n",
    "            pad1 = pad_total // 2\n",
    "            pad2 = pad_total - pad1\n",
    "            pad1 += extra\n",
    "            pad2 += extra\n",
    "        if dim == 'f':\n",
    "            fp1, fp2 = pad1, pad2\n",
    "        elif dim == 'h':\n",
    "            hp1, hp2 = pad1, pad2\n",
    "        elif dim == 'w':\n",
    "            wp1, wp2 = pad1, pad2\n",
    "    return torch.nn.functional.pad(\n",
    "        x,\n",
    "        pad=(wp1, wp2, hp1, hp2, fp1, fp2),\n",
    "        mode=\"reflect\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d24b72a7-8f04-40ec-8556-de62e892d93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='90' class='' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [90/90 04:07&lt;00:00 PSNR: 33.606770833333336, CR:72.41691412074336]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PSNR_list = []\n",
    "CR_list = []\n",
    "pb = progress_bar(dataset['train'])\n",
    "total_time = 0\n",
    "for sample in pb:\n",
    "    \n",
    "    video = sample['video']\n",
    "    len_video = len(video)\n",
    "    xr = video.get_batch(range(len_video))\n",
    "    xr = einops.rearrange(xr, 'f h w c -> c f h w')\n",
    "    x = []\n",
    "    for i_frame in range(n_frames):\n",
    "        frame = xr[:, i_frame]\n",
    "        pil_img = to_pil_image(frame)\n",
    "        resized_img = pil_img.resize((target_w, target_h))\n",
    "        tensor_frame = pil_to_tensor(resized_img).unsqueeze(1)\n",
    "        x.append(tensor_frame)\n",
    "    x = torch.cat(x, dim=1).unsqueeze(0)\n",
    "    x = x / 127.5 - 1.0\n",
    "    x = x.to(device).to(torch.bfloat16)\n",
    "    x_orig = x.clone()\n",
    "    x = pad3d(x, p=config.F, extra=0, small_dim_mode=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t0 = time.time()\n",
    "        z = model.encode(x)\n",
    "        latent = model.quantize.compand(z).round().to(torch.bfloat16)\n",
    "        x_hat = model.decode(latent).clamp(-1,1)\n",
    "        dt = time.time() - t0\n",
    "        total_time += dt\n",
    "    \n",
    "    x_cropped = []\n",
    "    x_hat_cropped  = []\n",
    "    cc = CenterCrop((target_h,target_w))\n",
    "    for i_frame in range(n_frames):\n",
    "        x_hat_cropped.append(cc(x_hat[:,:,i_frame]).unsqueeze(2))\n",
    "    x_hat = torch.cat(x_hat_cropped,dim=2)\n",
    "    \n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    PSNR = []\n",
    "    for i_frame in range(x_orig_01.shape[2]):\n",
    "        mse = torch.nn.functional.mse_loss(x_orig_01[0, :, i_frame], x_hat_01[0, :, i_frame])\n",
    "        PSNR.append(-10 * mse.log10().item())\n",
    "    PSNR_list.append(PSNR)\n",
    "    \n",
    "    size_bytes = 0\n",
    "    t0 = time.time()\n",
    "    for chunk in latent_to_pil(einops.rearrange(latent[0], 'c f h w -> f c h w').cpu(),n_bits=8,C=3):\n",
    "        buff = io.BytesIO()\n",
    "        chunk.save(buff,format='webp',lossless=True)\n",
    "        size_bytes += len(buff.getbuffer())\n",
    "    dt = time.time() - t0\n",
    "    total_time += dt\n",
    "    CR_list.append(x_orig.numel()/size_bytes)\n",
    "\n",
    "    pb.comment = (f\"PSNR: {np.mean(PSNR)}, CR:{CR_list[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7d720-446a-4f4d-89cc-6da177742916",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ffff1-f52a-48a0-b5d9-0b8d82c4d2f6",
   "metadata": {},
   "source": [
    "24x240x240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c14d64e-16fb-45f8-b9da-28724f464781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(27.115885416666668)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7635326-7ef3-4cde-a44e-5cc31bedf6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(67.60885125372157)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_cr) for per_frame_cr in CR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3338b93-460b-4499-a2f9-29feac4fd538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.95044907525985"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_w*target_h*24*90/total_time/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e525c-cfa0-47ab-8dcd-c06fd9a72db4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0cada-1b86-4845-b460-7b97495255b3",
   "metadata": {},
   "source": [
    "24x432x240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd957d44-1828-4931-8459-9d5cc1920962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(25.338541666666668)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea7879e-a9a7-415e-bd53-fecae3b53bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(93.7856267152748)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_cr) for per_frame_cr in CR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9a5fa97-b7df-4d7a-a283-fa37d79cca10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.01907287532129"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_w*target_h*24*90/total_time/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40fed9-0b2d-468b-b1b3-587edc97b104",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb264f16-2e4e-4dd8-bec4-528d7d119510",
   "metadata": {},
   "source": [
    "540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2810397-75c8-4b0c-9ff4-fb923a9fd082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(27.734375)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd58ab8-3ab0-45ba-8b7d-55a6a954e57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(78.7022754068909)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_cr) for per_frame_cr in CR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "880d7ea8-47b1-432a-9307-cffa812cc0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.96029137687159"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_w*target_h*24*90/total_time/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b5702-4dc0-4d6c-9d04-1f3b42287f16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42fea69-cb3c-40c4-957a-18ffa2e69acb",
   "metadata": {},
   "source": [
    "784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb6a5f09-86c7-4e99-a1eb-51aac5c42c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(32.708333333333336)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1feba41c-0e59-45fc-ac8d-6a360cf4fcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(68.16791729653269)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_cr) for per_frame_cr in CR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2459b6e1-5425-4e6b-9591-15394278247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.5282218012801"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_w*target_h*24*90/total_time/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3d640-dc53-4277-b073-63d1c4812f51",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc44bd8-823f-448e-aba6-eb70caa74114",
   "metadata": {},
   "source": [
    "784->1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d6d0c58-423e-4e23-8012-f43d0b4bb395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(33.606770833333336)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86820353-121f-4342-886a-32d51a7f3143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(72.18689670043877)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_cr) for per_frame_cr in CR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ca83d6d-d1fc-4899-8cbf-f28f65566c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.52410005860598"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_w*target_h*24*90/total_time/1e6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
