{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b138243-5dd9-4ac0-8ea1-5461bcb85e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "import random\n",
    "from timm.optim import Mars\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import HTML\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import CenterCrop, RandomCrop\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from decord import VideoReader\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550eece5-fccd-4fba-ba81-f81f73a3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/davis\").cast_column('video',datasets.Video()).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212dc070-e81c-46ae-a394-b476d4010a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../../hf/dance/video_f8c48_24f.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10fe555b-140d-4049-b2ab-5b0f73dac5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def decode_chunked(l,m,c,d,b,s,o,u):\n",
    "    t,h,w=l.shape[2:]\n",
    "    p=[x*c.F for x in [t,h,w]]\n",
    "    x=torch.zeros([1,3]+p,device=d)\n",
    "    n=[max(((x-b)//s)+1,1)for x in [t,h,w]]\n",
    "    for k in product(*[range(x)for x in n]):\n",
    "        st=[k[i]*s if n[i]>1 else 0 for i in range(3)]\n",
    "        e=[min(st[i]+b,[t,h,w][i])for i in range(3)]\n",
    "        sz=[e[i]-st[i]for i in range(3)]\n",
    "        pl=[(b-sz[i])//2 if sz[i]<b else 0 for i in range(3)]\n",
    "        pr=[b-sz[i]-pl[i] if sz[i]<b else 0 for i in range(3)]\n",
    "        sc=[o*u if n[i]>1 else pl[i]*u for i in range(3)]\n",
    "        ec=[sc[i]+(s if n[i]>1 else sz[i])*u for i in range(3)]\n",
    "        so=[(st[i]+o if n[i]>1 else 0)*u for i in range(3)]\n",
    "        eo=[so[i]+(s if n[i]>1 else sz[i])*u for i in range(3)]\n",
    "        lb=l[:,:,st[0]:e[0],st[1]:e[1],st[2]:e[2]]\n",
    "        pd=tuple(pl[2-i//2] if i%2==0 else pr[2-i//2] for i in range(6))\n",
    "        if any(pd):lb=torch.nn.functional.pad(lb,pd,mode=\"reflect\")\n",
    "        with torch.no_grad():xb=m.decode(lb).clamp(-1,1)\n",
    "        cp=xb[:,:,sc[0]:ec[0],sc[1]:ec[1],sc[2]:ec[2]]\n",
    "        x[:,:,so[0]:eo[0],so[1]:eo[1],so[2]:eo[2]]=cp\n",
    "    return x,*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b72a7-8f04-40ec-8556-de62e892d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 2\n",
    "stride = 5\n",
    "block_size = stride + 2 * overlap\n",
    "target_w = 1920\n",
    "target_h = 1080\n",
    "upsampling_factor = config.F\n",
    "overlap_pixels = overlap * upsampling_factor  # e.g., 4 * 8 = 32\n",
    "central_size_pixels = stride * upsampling_factor  # e.g., 16 * 8 = 128\n",
    "\n",
    "def pad3d(x, p, extra, small_dim_mode):\n",
    "    b, c, f, h, w = x.shape\n",
    "    for dim, size in zip(['f', 'h', 'w'], [f, h, w]):\n",
    "        if small_dim_mode and size < p:\n",
    "            pad1 = extra\n",
    "            pad2 = extra\n",
    "        else:\n",
    "            t = math.ceil(size / p) * p\n",
    "            pad_total = t - size\n",
    "            pad1 = pad_total // 2\n",
    "            pad2 = pad_total - pad1\n",
    "            pad1 += extra\n",
    "            pad2 += extra\n",
    "        if dim == 'f':\n",
    "            fp1, fp2 = pad1, pad2\n",
    "        elif dim == 'h':\n",
    "            hp1, hp2 = pad1, pad2\n",
    "        elif dim == 'w':\n",
    "            wp1, wp2 = pad1, pad2\n",
    "    return torch.nn.functional.pad(\n",
    "        x,\n",
    "        pad=(wp1, wp2, hp1, hp2, fp1, fp2),\n",
    "        mode=\"reflect\"\n",
    "    )\n",
    "\n",
    "PSNR_list = []\n",
    "CR_list = []\n",
    "pb = progress_bar(dataset['train'])\n",
    "for sample in pb:\n",
    "    video = sample['video']\n",
    "    len_video = len(video)\n",
    "    xr = video.get_batch(range(len_video))\n",
    "    xr = einops.rearrange(xr, 'f h w c -> c f h w')\n",
    "    x = []\n",
    "    for i_frame in range(xr.shape[1]):\n",
    "        frame = xr[:, i_frame]\n",
    "        pil_img = to_pil_image(frame)\n",
    "        resized_img = pil_img.resize((target_w, target_h))\n",
    "        tensor_frame = pil_to_tensor(resized_img).unsqueeze(1)\n",
    "        x.append(tensor_frame)\n",
    "    x = torch.cat(x, dim=1).unsqueeze(0)\n",
    "    x = x / 127.5 - 1.0\n",
    "    x = x.to(device)\n",
    "    x_orig = x.clone()\n",
    "    x = pad3d(x, p=config.F*stride, extra=config.F*overlap, small_dim_mode=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x)\n",
    "        latent = model.quantize.compand(z).round()\n",
    "\n",
    "    # Decode using the function\n",
    "    x_hat, F_padded, H_padded, W_padded = decode_chunked(\n",
    "        l=latent,\n",
    "        m=model,\n",
    "        c=config,\n",
    "        d=device,\n",
    "        b=block_size,\n",
    "        s=stride,\n",
    "        o=overlap,\n",
    "        u=config.F\n",
    "    )\n",
    "    \n",
    "    # Crop to original dimensions\n",
    "    f_start = (F_padded - len_video) // 2\n",
    "    h_start = (H_padded - target_h) // 2\n",
    "    w_start = (W_padded - target_w) // 2\n",
    "    x_hat = x_hat[:, :,\n",
    "                  f_start:f_start + len_video,\n",
    "                  h_start:h_start + target_h,\n",
    "                  w_start:w_start + target_w]\n",
    "    x_hat = x_hat.clamp(-1, 1)\n",
    "\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    PSNR = []\n",
    "    for i_frame in range(x_orig_01.shape[2]):\n",
    "        mse = torch.nn.functional.mse_loss(x_orig_01[0, :, i_frame], x_hat_01[0, :, i_frame])\n",
    "        PSNR.append(-10 * mse.log10().item())\n",
    "    PSNR_list.append(PSNR)\n",
    "\n",
    "    size_bytes = 0\n",
    "    for chunk in latent_to_pil(einops.rearrange(z[0], 'c f h w -> f c h w').cpu(),n_bits=8,C=3):\n",
    "        buff = io.BytesIO()\n",
    "        chunk.save(buff,format='webp',lossless=True)\n",
    "        size_bytes += len(buff.getbuffer())\n",
    "    CR_list.append(x_orig.numel()/size_bytes)\n",
    "\n",
    "    pb.comment = (f\"PSNR: {np.mean(PSNR)}, CR:{CR_list[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96841406-1851-46fa-b966-818338cb296d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='90' class='' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [90/90 1:51:58&lt;00:00 PSNR: 26.504651440514458, CR:50.255824725055746]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(pb.progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71eae643-a1c6-4815-a836-21139cd7cae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.30968318553215"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(CR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df9260eb-fa89-437d-b956-9ccd4387672a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.522737713120044"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
