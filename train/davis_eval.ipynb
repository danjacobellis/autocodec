{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b138243-5dd9-4ac0-8ea1-5461bcb85e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "import random\n",
    "from timm.optim import Mars\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import HTML\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import CenterCrop, RandomCrop\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from decord import VideoReader\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550eece5-fccd-4fba-ba81-f81f73a3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/davis\").cast_column('video',datasets.Video()).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212dc070-e81c-46ae-a394-b476d4010a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ce6a09-9523-4724-a230-480333b4b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 4\n",
    "stride = 16\n",
    "block_size = stride + 2 * overlap\n",
    "target_w = 1920\n",
    "target_h = 1080\n",
    "upsampling_factor = config.F\n",
    "overlap_pixels = overlap * upsampling_factor  # e.g., 4 * 8 = 32\n",
    "central_size_pixels = stride * upsampling_factor  # e.g., 16 * 8 = 128\n",
    "\n",
    "def pad3d(x, p, extra):\n",
    "    b, c, f, h, w = x.shape\n",
    "    t = math.ceil(f / p) * p\n",
    "    fp1 = (t - f) // 2\n",
    "    fp2 = (t - f) - fp1\n",
    "    t = math.ceil(h / p) * p\n",
    "    hp1 = (t - h) // 2\n",
    "    hp2 = (t - h) - hp1\n",
    "    t = math.ceil(w / p) * p\n",
    "    wp1 = (t - w) // 2\n",
    "    wp2 = (t - w) - wp1\n",
    "    return torch.nn.functional.pad(x, pad=(\n",
    "        wp1+extra,\n",
    "        wp2+extra,\n",
    "        hp1+extra,\n",
    "        hp2+extra,\n",
    "        fp1+extra,\n",
    "        fp2+extra\n",
    "    ), mode=\"reflect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b72a7-8f04-40ec-8556-de62e892d93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.11% [1/90 00:46&lt;1:09:24 PSNR: 25.542215981134554]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PSNR_list = []\n",
    "pb = progress_bar(dataset['train'])\n",
    "for sample in pb:\n",
    "    video = sample['video']\n",
    "    len_video = len(video)\n",
    "    xr = video.get_batch(range(len_video))\n",
    "    xr = einops.rearrange(xr, 'f h w c -> c f h w')\n",
    "    x = []\n",
    "    for i_frame in range(xr.shape[1]):\n",
    "        frame = xr[:, i_frame]\n",
    "        pil_img = to_pil_image(frame)\n",
    "        resized_img = pil_img.resize((target_w, target_h))\n",
    "        tensor_frame = pil_to_tensor(resized_img).unsqueeze(1)\n",
    "        x.append(tensor_frame)\n",
    "    x = torch.cat(x, dim=1).unsqueeze(0)\n",
    "    x = x / 127.5 - 1.0\n",
    "    x = x.to(device)\n",
    "    x_orig = x.clone()\n",
    "    x = pad3d(x, p=config.F * stride, extra=config.F * overlap)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x)\n",
    "        latent = model.quantize.compand(z).round()\n",
    "    \n",
    "    \n",
    "    # Latent dimensions\n",
    "    T_l, H_l, W_l = latent.shape[2], latent.shape[3], latent.shape[4]\n",
    "    F_padded = T_l * config.F\n",
    "    H_padded = H_l * config.F\n",
    "    W_padded = W_l * config.F\n",
    "    \n",
    "    # Initialize output tensor\n",
    "    x_hat = torch.zeros(1, 3, F_padded, H_padded, W_padded, device=device)\n",
    "    \n",
    "    # Number of blocks\n",
    "    num_blocks_t = ((T_l - block_size) // stride) + 1\n",
    "    num_blocks_h = ((H_l - block_size) // stride) + 1\n",
    "    num_blocks_w = ((W_l - block_size) // stride) + 1\n",
    "    \n",
    "    for k_t in range(num_blocks_t):\n",
    "        for k_h in range(num_blocks_h):\n",
    "            for k_w in range(num_blocks_w):\n",
    "                t_start = k_t * stride\n",
    "                h_start = k_h * stride\n",
    "                w_start = k_w * stride\n",
    "                latent_block = latent[:, :, t_start:t_start + block_size,\n",
    "                                           h_start:h_start + block_size,\n",
    "                                           w_start:w_start + block_size]\n",
    "                with torch.no_grad():\n",
    "                    x_hat_block = model.decode(latent_block).clamp(-1, 1)\n",
    "                central_part = x_hat_block[:, :,\n",
    "                                           overlap_pixels:overlap_pixels + central_size_pixels,\n",
    "                                           overlap_pixels:overlap_pixels + central_size_pixels,\n",
    "                                           overlap_pixels:overlap_pixels + central_size_pixels]\n",
    "                t_pixel_start = t_start * upsampling_factor\n",
    "                h_pixel_start = h_start * upsampling_factor\n",
    "                w_pixel_start = w_start * upsampling_factor\n",
    "                x_hat[:, :,\n",
    "                      t_pixel_start + overlap_pixels:t_pixel_start + overlap_pixels + central_size_pixels,\n",
    "                      h_pixel_start + overlap_pixels:h_pixel_start + overlap_pixels + central_size_pixels,\n",
    "                      w_pixel_start + overlap_pixels:w_pixel_start + overlap_pixels + central_size_pixels] = central_part\n",
    "    f_start = (F_padded - len_video) // 2\n",
    "    h_start = (H_padded - target_h) // 2\n",
    "    w_start = (W_padded - target_w) // 2\n",
    "    x_hat = x_hat[:, :,\n",
    "                  f_start:f_start + len_video,\n",
    "                  h_start:h_start + target_h,\n",
    "                  w_start:w_start + target_w]\n",
    "    x_hat = x_hat.clamp(-1, 1)\n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    PSNR = []\n",
    "    for i_frame in range(x_orig_01.shape[2]):\n",
    "        mse = torch.nn.functional.mse_loss(x_orig_01[0, :, i_frame], x_hat_01[0, :, i_frame])\n",
    "        PSNR.append(-10 * mse.log10().item())\n",
    "    PSNR_list.append(PSNR)\n",
    "\n",
    "    pb.comment = (f\"PSNR: {np.mean(PSNR)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
