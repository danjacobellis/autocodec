{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac028e-ffa0-4ed2-9b2a-1ec0a4c1e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/danjacobellis/dance/resolve/main/video_f8c48_24f.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from timm.optim import Mars\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import HTML\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import CenterCrop, RandomCrop\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from decord import VideoReader\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550eece5-fccd-4fba-ba81-f81f73a3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dataset = datasets.load_dataset(\"danjacobellis/davis\").cast_column('video',datasets.Video()).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212dc070-e81c-46ae-a394-b476d4010a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('../../hf/dance/video_f8c48_24f_784p_.pth', map_location=\"cpu\",weights_only=False)\n",
    "checkpoint = torch.load('checkpoint_f8c48_784_e1.pth', map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = AutoCodecND(\n",
    "    dim=3,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = config.encoder_depth,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ").to(device).to(torch.bfloat16)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb2bf0e-b731-4c11-9201-ac26cbf1a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad3d(x, p, extra, small_dim_mode):\n",
    "    b, c, f, h, w = x.shape\n",
    "    extra_f, extra_h, extra_w = extra  # Unpack the extra tuple for each dimension\n",
    "    \n",
    "    for dim, size, extra_pad in zip(['f', 'h', 'w'], [f, h, w], [extra_f, extra_h, extra_w]):\n",
    "        if small_dim_mode and size < p:\n",
    "            pad1 = extra_pad\n",
    "            pad2 = extra_pad\n",
    "        else:\n",
    "            t = math.ceil(size / p) * p\n",
    "            pad_total = t - size\n",
    "            pad1 = pad_total // 2\n",
    "            pad2 = pad_total - pad1\n",
    "            pad1 += extra_pad\n",
    "            pad2 += extra_pad\n",
    "        if dim == 'f':\n",
    "            fp1, fp2 = pad1, pad2\n",
    "        elif dim == 'h':\n",
    "            hp1, hp2 = pad1, pad2\n",
    "        elif dim == 'w':\n",
    "            wp1, wp2 = pad1, pad2\n",
    "            \n",
    "    return torch.nn.functional.pad(\n",
    "        x,\n",
    "        pad=(wp1, wp2, hp1, hp2, fp1, fp2),\n",
    "        mode=\"reflect\"\n",
    "    )\n",
    "\n",
    "def center_crop_3d(x, f, h, w):\n",
    "    assert x.ndim == 5\n",
    "    _, _, F, H, W = x.shape\n",
    "    front = (F - f) // 2\n",
    "    back  = front + f\n",
    "    top   = (H - h) // 2\n",
    "    bottom = top + h\n",
    "    left  = (W - w) // 2\n",
    "    right = left + w\n",
    "    return x[:, :, front:back, top:bottom, left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24b72a7-8f04-40ec-8556-de62e892d93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='90' class='' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [90/90 20:19&lt;00:00 PSNR: 29.45529513888889, CR:56.83683970757867]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_w = 1920\n",
    "target_h = 1080\n",
    "PSNR_list = []\n",
    "CR_list = []\n",
    "pb = progress_bar(dataset['train'])\n",
    "total_time = 0\n",
    "for sample in pb:\n",
    "    \n",
    "    video = sample['video']\n",
    "    len_video = len(video)\n",
    "    xr = video.get_batch(range(len_video))\n",
    "    xr = einops.rearrange(xr, 'f h w c -> c f h w')\n",
    "    x = []\n",
    "    for i_frame in range(len_video):\n",
    "        frame = xr[:, i_frame]\n",
    "        pil_img = to_pil_image(frame)\n",
    "        resized_img = pil_img.resize((target_w, target_h))\n",
    "        tensor_frame = pil_to_tensor(resized_img).unsqueeze(1)\n",
    "        x.append(tensor_frame)\n",
    "    x = torch.cat(x, dim=1).unsqueeze(0)\n",
    "    x = x / 127.5 - 1.0\n",
    "    x = x.to(device).to(torch.bfloat16)\n",
    "    x_orig = x.clone()\n",
    "    x = pad3d(x, p=config.F, extra=(8,0,0), small_dim_mode=True)\n",
    "    # encode\n",
    "    with torch.no_grad():\n",
    "        t0 = time.time()\n",
    "        z = model.encode(x)\n",
    "        latent = model.quantize.compand(z).round().to(torch.bfloat16)\n",
    "        dt = time.time() - t0\n",
    "        total_time += dt\n",
    "\n",
    "    # decode\n",
    "    x_hat = []\n",
    "    for i_chunk in range(latent.shape[2]-2):\n",
    "        i1 = i_chunk; i2 = i_chunk+3\n",
    "        latent_chunk = latent[:,:,i1:i2,:,:]\n",
    "        with torch.no_grad():\n",
    "            x_hat.append(model.decode(latent_chunk)[:,:,8:16].clamp(-1,1))\n",
    "    x_hat = torch.cat(x_hat,dim=2)\n",
    "    \n",
    "    _,_,f,h,w = x_orig.shape\n",
    "    x_hat = center_crop_3d(x_hat,f,h,w)\n",
    "    \n",
    "    x_orig_01 = x_orig / 2 + 0.5\n",
    "    x_hat_01 = x_hat / 2 + 0.5\n",
    "    PSNR = []\n",
    "    for i_frame in range(x_orig_01.shape[2]):\n",
    "        mse = torch.nn.functional.mse_loss(x_orig_01[0, :, i_frame], x_hat_01[0, :, i_frame])\n",
    "        PSNR.append(-10 * mse.log10().item())\n",
    "    PSNR_list.append(PSNR)\n",
    "    \n",
    "    size_bytes = 0\n",
    "    t0 = time.time()\n",
    "    for chunk in latent_to_pil(einops.rearrange(latent[0], 'c f h w -> f c h w').cpu(),n_bits=8,C=3):\n",
    "        buff = io.BytesIO()\n",
    "        chunk.save(buff,format='webp',lossless=True)\n",
    "        size_bytes += len(buff.getbuffer())\n",
    "    dt = time.time() - t0\n",
    "    total_time += dt\n",
    "    CR_list.append(x_orig.numel()/size_bytes)\n",
    "\n",
    "    pb.comment = (f\"PSNR: {np.mean(PSNR)}, CR:{CR_list[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6d0c58-423e-4e23-8012-f43d0b4bb395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(22.34375)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([np.min(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9805fc32-2dd1-49cc-8a36-f211376a2e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(30.48656425619671)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3396bfc8-b269-4877-897e-25dcd0ee9a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(38.75)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([np.max(per_frame_psnr) for per_frame_psnr in PSNR_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
